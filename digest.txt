Directory structure:
└── market_research_tool/
    ├── README.md
    ├── build.sh
    ├── celery_worker.py
    ├── docker-compose.yml
    ├── jobs.db
    ├── make_ingest.py
    ├── requirements.txt
    ├── .env.example
    ├── api/
    │   ├── __init__.py
    │   ├── auth.py
    │   ├── logging_config.py
    │   ├── models.py
    │   ├── server.py
    │   ├── sheets_logger.py
    │   └── __pycache__/
    ├── database/
    │   ├── __init__.py
    │   ├── models.py
    │   ├── session.py
    │   └── __pycache__/
    └── src/
        ├── __init__.py
        ├── config.py
        ├── constants.py
        ├── main.py
        ├── phase1_planner.py
        ├── phase2_searcher.py
        ├── phase3_intermediate_synthesizer.py
        ├── phase4_extractor.py
        ├── phase5_final_synthesizer.py
        ├── rag_uploader.py
        ├── tasks.py
        └── __pycache__/

================================================
File: README.md
================================================
# Market Research Automation Tool

A modular pipeline that automates desk research for the paint & coatings industry. The system ingests a natural‑language brief, plans targeted search queries, collects and synthesises web content, and returns both an executive‑level report and structured data (news, Patents articles, conference events, Legalnews). It is exposed through a **FastAPI** micro‑service with **database-backed job persistence** and features an **automated RAG uploader** for post-research querying.

---

## 1 — Key Features

| Capability                               | Description                                                                                                                                                             |
| ---------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Bucketed query planning**              | Gemini analyses the user brief and produces site‑restricted Google CSE queries, organised into *News*, *Patents*, *Conference*, *Legalnews* and *General* buckets.         |
| **Parallel custom‑search harvesting**    | Parameterised concurrency for Google CSE with per‑thread service instances to avoid throttling.                                                                         |
| **Context‑aware synthesis**              | Intermediate sub‑reports per 15‑URL batch; final executive report merges these into a cohesive analysis.                                                                |
| **Targeted structured extraction**       | Separate Gemini extractor returns normalised JSON objects (type, title, summary, date, source\_url) only from specific URLs allocated for structured data.                  |
| **Database-backed job persistence**      | A local **SQLite** database tracks the state of all submitted jobs (pending, running, completed, failed), ensuring state is not lost on server restart.                 |
| **Automated RAG collection creation**    | On job completion, automatically creates a dedicated RAG collection, converts all reports and data to PDF, and uploads them for querying.                                 |
| **Post-research RAG querying**           | API endpoints allow for conversational querying of a completed job's RAG collection, maintaining chat context for follow-up questions.                                   |
| **Configurable content freshness**       | Central `RECENT_YEARS` constant filters out search results, extractions, and analysis older than a defined period to ensure relevance.                                   |
| **Async & multithreaded orchestration**  | Asyncio + `ThreadPoolExecutor` for optimal IO‑bound and CPU‑bound step overlap.                                                                                         |
| **REST API**                             | `POST /api/research` (submit), `GET /api/research/...` (poll/retrieve), and `POST /api/rag/query` (ask).                                                                 |
| **Extensible design**                    | Each pipeline phase is its own module; easy to swap LLMs, add new data sources, or introduce caching.                                                                   |

---

## 2 — Directory Structure

```text
market_research_tool/
├─ api/                          # FastAPI service
│  ├─ models.py                  # Pydantic request / response models
│  └─ server.py                  # HTTP endpoints, job persistence, & RAG orchestration
├─ database/                     # SQLAlchemy models & session management
│  ├─ models.py                  # Defines the 'jobs' table schema
│  └─ session.py                 # DB engine and session configuration
├─ src/                          # Core pipeline implementation
│  ├─ config.py                  # Environment variable loading & validation
│  ├─ constants.py               # Centralised runtime limits
│  ├─ main.py                    # Orchestrator (execute_research_pipeline)
│  ├─ phase1_planner.py
│  ├─ phase2_searcher.py
│  ├─ phase3_intermediate_synthesizer.py
│  ├─ phase4_extractor.py
│  ├─ phase5_final_synthesizer.py
│  └─ rag_uploader.py            # Converts artifacts to PDF and uploads to RAG system
├─ reports/                      # Markdown reports (auto‑generated)
├─ extractions/                  # Structured JSON extractions (auto‑generated)
├─ jobs.db                       # SQLite database for job persistence
├─ requirements.txt
└─ .env.example                  # Template for secrets
```

---

## 3 — Installation

### 3.1 Prerequisites

*   Python 3.11+
*   A Google Cloud account with **Custom Search JSON API** enabled
*   Gemini API access (or adjust to another LLM)
*   Access to a RAG API service (for the upload and query features)

### 3.2 Set‑up

```bash
# clone
$ git clone <repo-url> market_research_tool && cd $_

# create isolated environment
$ python -m venv venv
$ source venv/bin/activate

# install dependencies
(venv) $ pip install -r requirements.txt

# copy secrets template
(venv) $ cp .env.example .env

# edit .env with your API keys, CSE ID, and RAG service details
```
> **Note:** The first time you run the API service, it will automatically create the `jobs.db` file.

---

## 4 — Running the Pipeline

### 4.1 CLI (one‑off batch)

```bash
(venv) $ python -m src.main <<EOF
Show me the latest innovations in Weatherability of Decorative Coatings.
What trends are emerging in the Sustainability of industrial coatings in 2025?
Find recent conferences or Patents discussing Scuff‑Resistance in coatings.
EOF
```

Progress is logged to STDOUT; final artefacts appear in `reports/` and `extractions/`.

### 4.2 API Service

```bash
(venv) $ uvicorn api.server:app --reload
```

Interactive OpenAPI documentation becomes available at [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs).

---

## 5 — API Reference

### 5.1 Submit a Job

`POST /api/research`

Submits a new research job. The `upload_to_rag` flag controls whether the results are sent to the RAG system upon completion.

```jsonc
{
  "query": "- Show me the latest innovations in Weatherability of Decorative Coatings.\n- What trends are emerging in the Sustainability of industrial coatings in 2025?\n- Find recent conferences or Patents discussing Scuff-Resistance in coatings.\n\nSearch tags/topics – Product, coating, architectural or similar.\nDatasources/URLs (https://www.paint.org/, https://www.coatingsworld.com/, https://www.pcimag.com/)",
  "upload_to_rag": true
}
```

Response `202 Accepted`

```json
{
  "job_id": "fbadce0d-f51a-4e1d-83ad-cd971c7ba4c7",
  "status": "pending",
  "status_url": "http://127.0.0.1:8000/api/research/status/fbadce0d-f51a-4e1d-83ad-cd971c7ba4c7",
  "result_url": "http://127.0.0.1:8000/api/research/result/fbadce0d-f51a-4e1d-83ad-cd971c7ba4c7"
}
```

### 5.2 Check Status

`GET /api/research/status/{job_id}` → `JobStatusResponse`

Polls the status of a job. The message will include RAG upload status if applicable.

```json
{
  "job_id": "...",
  "status": "completed",
  "message": "Job status is completed. RAG upload successful (Collection: orgid_research_job_...)"
}
```

### 5.3 Retrieve Result

`GET /api/research/result/{job_id}` → `ResearchResult`

Retrieves the final report and structured data. The `metadata` block contains RAG info if it was requested.

-   `final_report_markdown` — complete executive report
-   `extracted_data` — categorised list (`News` | `Patents` | `Conference` | `Legalnews` | `Other`)
-   `metadata` — timing, counts, and RAG info

```jsonc
// Snippet of the response structure
{
  "job_id": "...",
  "status": "completed",
  "original_query": "...",
  "final_report_markdown": "# Executive Summary\n...",
  "extracted_data": { "...": [] },
  "metadata": {
    "timestamp": "...",
    "extraction_summary": { "...": 0 },
    "rag_info": { // Included if upload_to_rag was true
      "upload_requested": true,
      "rag_status": "uploaded",
      "collection_name": "orgid_research_job_fbadce0d_...",
      "rag_error": null
    }
  }
}
```

### 5.4 Get RAG Collection Info

`GET /api/research/{job_id}/rag` → `RAGCollectionInfo`

Returns detailed information about the RAG collection associated with a specific job, including whether it's ready to be queried.

Response `200 OK`
```json
{
    "job_id": "fbadce0d-f51a-4e1d-83ad-cd971c7ba4c7",
    "rag_status": "uploaded",
    "collection_name": "orgid_research_job_fbadce0d_f51a_4e1d_83ad_cd971c7ba4c7",
    "rag_error": null,
    "can_query": true
}
```

### 5.5 Query the RAG Collection

`POST /api/rag/query` → `RAGQueryResponse`

Ask a question to a specific RAG collection. The system automatically manages chat history in the database for conversational follow-up.

```jsonc
// Request
{
    "collection_name": "orgid_research_job_fbadce0d_f51a_4e1d_83ad_cd971c7ba4c7",
    "question": "Which companies are leading in scuff-resistant coating technologies?"
}
```

Response `200 OK`
```json
{
    "collection_name": "orgid_research_job_fbadce0d_...",
    "question": "Which companies are leading in scuff-resistant coating technologies?",
    "answer": {
        "response": "Based on the provided documents, the key players in scuff-resistant coating technologies include...",
        "citations": [ /* ... */ ]
    }
}
```

---

## 6 — Configuration & Tuning

### 6.1 Pipeline Constants (`src/constants.py`)

This file centralises critical limits for the research pipeline:

| Constant                 | Purpose                                                                    | Default |
| ------------------------ | -------------------------------------------------------------------------- | ------- |
| `MAX_SEARCH_RESULTS`     | Google CSE results per query                                               | 4       |
| `MAX_SEARCH_WORKERS`     | Threads for CSE calls                                                      | 9       |
| `MAX_GENERAL_FOR_REPORT` | URLs allowed in the **General** bucket (feeds Phase‑3/5)                   | 18      |
| `MAX_PER_BUCKET_EXTRACT` | URLs per specialised bucket (feeds Phase‑4)                                | 9       |
| `EXTRACT_BATCH_SIZE`     | URLs processed per Gemini extract batch                                    | 18      |
| `MAX_GEMINI_PARALLEL`    | Concurrent Gemini extract calls                                            | 9       |
| `RECENT_YEARS`           | Filters out content older than N years to maintain freshness               | 2       |

### 6.2 Environment Variables (`.env`)
The `.env` file holds all necessary secrets. In addition to Google keys, the RAG uploader requires its own configuration:
-   `GEMINI_API_KEY`, `GOOGLE_API_KEY`, `GOOGLE_CSE_ID`: For core research.
-   `RAG_API_BASE_URL`, `RAG_API_TOKEN`, `RAG_API_ORG_ID`: For the RAG uploader and query system.

---

## 7 — Logging & Artefacts

*   **Job State:** Persisted in the `jobs.db` SQLite database file.
*   **Intermediate sub‑reports:** `reports/intermediate_reports/`
*   **Final reports:** `reports/`
*   **Structured JSON extractions:** `extractions/`
*   **STDOUT logs:** Encapsulate phase boundaries, counts, durations and error traces; suitable for piping into a log aggregator.

---

## 8 — Extending the Pipeline

1.  **Add a new data source**: update `phase1_planner.py` prompt to include the domain, adjust CSE queries as required.
2.  **Swap LLM**: provide an alternative client and swap calls in phases; ensure streaming token semantics are preserved.
3.  **Customize RAG Behavior**: Modify prompts, PDF generation, and API logic in `src/rag_uploader.py`.
4.  **Introduce caching**: layer a Redis cache around `_execute_single_query` and `extract_data_from_single_url` to reduce API spend.
5.  **Dockerisation**: create a slim Python image, copy project, install requirements, expose port 8000. Mount a volume for reports/extractions and `jobs.db` if persistence across containers is required.

---

## 9 — Testing & Quality

```bash
# run unit tests (pytest recommended)
(venv) $ pytest -q

# static typing
(venv) $ mypy src/ api/ database/

# style guide (PEP‑8 via ruff)
(venv) $ ruff check .
```

---

## 10 — Contribution Guidelines

*   Fork the repository & create a feature branch.
*   Follow the existing module structure; each phase should remain independently testable.
*   If adding database columns, consider migration strategies.
*   Run all tests and linters before submitting a PR.
*   Document any public‑facing changes (API, constants) in this README.

---

## 11 — License

Distributed under the **MIT License**. See `LICENSE` for details.

---

## 12 — Acknowledgements

*   Google GenAI SDK & Gemini models
*   Google Custom Search JSON API
*   FastAPI & Uvicorn
*   SQLAlchemy
*   ReportLab
*   Paint.org, CoatingsWorld.com, PCImag.com — publicly accessible content utilised for demonstration purposes only.


================================================
File: build.sh
================================================
#!/usr/bin/env bash
# File: build.sh

# exit on error
set -o errexit

pip install --upgrade pip
pip install -r requirements.txt 


================================================
File: celery_worker.py
================================================
# File: celery_worker.py (in the root directory)
import os
from celery import Celery
from dotenv import load_dotenv

# Load .env file for local development
load_dotenv()

# --- MODIFIED FOR RENDER ---
# Render provides the REDIS_URL environment variable automatically when services are linked.
# We fall back to the local .env variable for development.
redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/0")

# Create the Celery app instance
# The first argument is the name of the current module.
# The `broker` argument specifies the URL of the message broker (Redis).
# The `backend` argument is used to store task results. We'll also use Redis for this.
celery_app = Celery(
    "tasks",
    broker=redis_url,
    backend=redis_url
)

# Optional: Configure Celery to automatically discover tasks
# It will look for tasks in a file named `tasks.py` inside any installed apps.
# For our structure, we will explicitly define the task path.
celery_app.conf.update(
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    timezone='UTC',
    enable_utc=True,
)

# This is where we will tell Celery where to find our task function.
# We will create this file in the next step.
celery_app.autodiscover_tasks(['src.tasks']) 


================================================
File: docker-compose.yml
================================================
# File: docker-compose.yml
version: '3.8'

services:
  db:
    image: postgres:15-alpine  # Use a specific, lightweight version
    container_name: market_intel_db
    restart: always
    environment:
      - POSTGRES_USER=market_user
      - POSTGRES_PASSWORD=supervity
      - POSTGRES_DB=market_intel_db
    ports:
      - "5432:5432" # Expose the DB port to your local machine
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data: # This ensures your data persists even if the container is removed


================================================
File: jobs.db
================================================
[Non-text file]


================================================
File: make_ingest.py
================================================
# make_ingest.py

import sys
import subprocess

def generate_digest_cli(source, output_file="digest.txt", exclude_exts=None):
    cmd = ["gitingest", source, "-o", output_file]
    
    # Always exclude reports directory
    exclusions = ["reports", "reports/*", "extractions", "extractions/*", "logs.txt", "__pycache__"]
    
    if exclude_exts:
        # Format extensions as "*.ext" and add to exclusions
        exclusions.extend(f"*{ext}" for ext in exclude_exts)

    if exclusions:
        patterns = ",".join(exclusions)
        cmd += ["-e", patterns]

    print("Running:", " ".join(cmd))

    try:
        subprocess.run(cmd, check=True)
        print(f"✅ Digest written to {output_file}")
    except subprocess.CalledProcessError as e:
        print("❌ Error during gitingest execution:", e)

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python make_ingest.py <path_or_url> [output_file] [excluded_exts...]")
        sys.exit(1)

    source = sys.argv[1]
    
    # Determine if second argument is an output file or an extension
    output_file = "digest.txt"
    exclude_exts = []

    if len(sys.argv) >= 3 and sys.argv[2].startswith(".") is False:
        output_file = sys.argv[2]
        exclude_exts = sys.argv[3:]
    else:
        exclude_exts = sys.argv[2:]

    generate_digest_cli(source, output_file, exclude_exts)



================================================
File: requirements.txt
================================================
annotated-types==0.7.0
anyio==4.9.0
cachetools==5.5.2
certifi==2025.6.15
charset-normalizer==3.4.2
google-api-core==2.25.1
google-api-python-client==2.172.0
google-auth==2.40.3
google-auth-httplib2==0.2.0
google-genai>=1.20.0,<2.0
googleapis-common-protos==1.70.0
h11==0.16.0
httpcore==1.0.9
reportlab
gunicorn==22.0.0
httplib2==0.22.0
httpx[http2]==0.28.1
celery==5.4.0
redis==5.0.4
passlib[bcrypt]==1.7.4
python-jose[cryptography]==3.3.0
psycopg2-binary==2.9.9
idna==3.10
sqlalchemy==2.0.31
proto-plus==1.26.1
protobuf==3.20.3
pyasn1==0.6.1
pyasn1_modules==0.4.2
pydantic==2.11.7
pydantic_core==2.33.2
pyparsing==3.2.3
python-dotenv==1.1.0
requests==2.32.4
rsa==4.9.1
sniffio==1.3.1
typing-inspection==0.4.1
typing_extensions==4.14.0
uritemplate==4.2.0
urllib3==2.4.0
websockets==15.0.1
fastapi==0.110.0
uvicorn[standard]==0.29.0
python-dateutil==2.9.0



================================================
File: .env.example
================================================
# =============================================================================
# MARKET INTELLIGENCE AGENT - Environment Configuration
# =============================================================================

# --- Google/Gemini API Configuration ---
GEMINI_API_KEY=your_gemini_api_key_here
GOOGLE_API_KEY=your_google_search_api_key_here
GOOGLE_CSE_ID=your_custom_search_engine_id_here

# --- RAG API Configuration ---
RAG_API_BASE_URL=https://your-rag-api-base-url.com
RAG_API_TOKEN=your_rag_api_token_here
RAG_API_ORG_ID=your_organization_id_here
RAG_API_USER_TYPE=pro
SECRET_KEY=your_super_secret_string_here_change_this_in_production_12345
SHEETS_WEB_APP_URL=the_url_you_just_copied
DATABASE_URL=postgres://
REDIS_URL=redis://

# =============================================================================


================================================
File: api/__init__.py
================================================



================================================
File: api/auth.py
================================================
# File: api/auth.py

from datetime import datetime, timedelta, timezone
from typing import Optional
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt
from passlib.context import CryptContext
import os
import uuid

# --- Configuration ---
SECRET_KEY = os.getenv("SECRET_KEY", "a_very_secret_key_that_you_should_change") # Add this to your .env
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7  # 7 days

# --- Password Hashing ---
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# --- OAuth2 Scheme ---
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/auth/token")

# --- Utility Functions ---
def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None) -> str:
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

# --- Dependency to Get Current User ---
# This will be used to protect our endpoints
from sqlalchemy.orm import Session
from database.session import SessionLocal
from database.models import User as DBUser

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

async def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)) -> DBUser:
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    
    user = db.query(DBUser).filter(DBUser.id == user_id).first()
    if user is None:
        raise credentials_exception
    return user 


================================================
File: api/logging_config.py
================================================
# File: api/logging_config.py (NEW FILE)
import logging
import sys

def setup_logging():
    """
    Configures logging for the application to output structured logs to stdout.
    """
    # Get the root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO) # Set the minimum level of logs to capture

    # Remove any existing handlers to avoid duplicate logs
    if root_logger.hasHandlers():
        root_logger.handlers.clear()

    # Create a handler to write to standard output (console)
    handler = logging.StreamHandler(sys.stdout)
    
    # Create a formatter that outputs logs in a structured, readable format
    # We include timestamp, log level, logger name, and the message.
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - [%(name)s] - %(message)s'
    )
    handler.setFormatter(formatter)

    # Add the handler to the root logger
    root_logger.addHandler(handler)

    # Set the logging level for libraries that are too verbose
    logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)

    # Log a confirmation message
    logging.info("Logging configured successfully.") 


================================================
File: api/models.py
================================================
# api/models.py
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field
from datetime import datetime

# --- Request Models ---

class ResearchRequest(BaseModel):
    """The user's initial request to start a research job."""
    query: str = Field(
        ...,
        description="The natural language query for the market research.",
        example="Show me the latest innovations in Weatherability of Decorative Coatings.",
        min_length=10,
        max_length=2000
    )
    upload_to_rag: bool = Field(
        default=True,
        description="If true, all generated artifacts will be uploaded to the internal RAG system for future querying."
    )

class RAGQueryRequest(BaseModel):
    """Request to query a RAG collection."""
    collection_name: str = Field(
        ..., 
        description="The name of the RAG collection to query (e.g., 'research_job_<uuid>')",
        example="research_job_12345678_1234_5678_9abc_123456789def"
    )
    question: str = Field(
        ..., 
        description="The question to ask the RAG system.",
        example="What are the latest innovations in coating technology?",
        min_length=5,
        max_length=1000
    )

# --- Response Models ---

class JobSubmissionResponse(BaseModel):
    """Response after submitting a job, providing the ID and status URL."""
    job_id: str
    status: str = "pending"
    status_url: str
    result_url: str


class JobStatusResponse(BaseModel):
    """Response for checking the status of a job."""
    job_id: str
    status: str = Field(..., description="Current job status: pending, running, completed, or failed")
    message: str = Field(..., description="Detailed status message including RAG upload status if applicable")
    stage: Optional[str] = Field(None, description="The current machine-readable stage of the pipeline")
    progress: Optional[int] = Field(None, description="An estimated progress percentage for the current stage")
    logs: Optional[List[str]] = Field(None, description="A list of the latest log messages from the job.")


class StructuredDataItem(BaseModel):
    """A single extracted item, like a news article or patent."""
    type: str = Field(..., description="Type of the item (News, Patents, Conference, etc.)")
    title: str = Field(..., description="Title of the item")
    summary: str = Field(..., description="Summary or description of the item")
    date: Optional[str] = Field(None, description="Publication or event date")
    source_url: str = Field(..., description="Original URL of the item")


class ExtractedData(BaseModel):
    """The categorized collection of all structured items."""
    News: List[StructuredDataItem] = Field(default_factory=list, description="News articles and updates")
    Patents: List[StructuredDataItem] = Field(default_factory=list, description="Patent filings and innovations")
    Conference: List[StructuredDataItem] = Field(default_factory=list, description="Conference proceedings and presentations")
    Legalnews: List[StructuredDataItem] = Field(default_factory=list, description="Legal news and regulatory updates")
    Other: List[StructuredDataItem] = Field(default_factory=list, description="Other miscellaneous items")


class RAGInfo(BaseModel):
    """Information about RAG upload status and collection."""
    upload_requested: bool = Field(..., description="Whether RAG upload was requested")
    rag_status: Optional[str] = Field(None, description="Status of RAG upload: pending, uploaded, failed")
    collection_name: Optional[str] = Field(None, description="Name of the created RAG collection")
    rag_error: Optional[str] = Field(None, description="Error message if RAG upload failed")


class ResearchResult(BaseModel):
    """The final, complete result of a research job."""
    job_id: str
    status: str
    original_query: str = Field(..., description="The original research query")
    final_report_markdown: str = Field(..., description="Complete final report in markdown format")
    extracted_data: ExtractedData = Field(..., description="Structured extracted data categorized by type")
    metadata: Dict[str, Any] = Field(..., description="Additional metadata including extraction stats and RAG info")


class RAGQueryResponse(BaseModel):
    """Response from querying a RAG collection."""
    collection_name: str
    question: str
    answer: Union[Dict[str, Any], str] = Field(..., description="The answer from the RAG system")
    
    class Config:
        # Allow for flexibility in the answer format
        extra = "allow"


class RAGCollectionInfo(BaseModel):
    """Information about a job's RAG collection."""
    job_id: str
    rag_status: str = Field(..., description="Status: pending, uploaded, failed, unknown")
    collection_name: Optional[str] = Field(None, description="Name of the RAG collection if available")
    rag_error: Optional[str] = Field(None, description="Error message if upload failed")
    can_query: bool = Field(..., description="Whether the collection is ready for querying")


# +++ NEW: Models for Job History +++
class JobHistoryItem(BaseModel):
    id: str
    original_query: str
    status: str
    created_at: datetime  # We'll use this for display and sorting

    class Config:
        from_attributes = True

class JobHistoryResponse(BaseModel):
    jobs: List[JobHistoryItem]


================================================
File: api/server.py
================================================
# api/server.py
import uuid
import os  # Add this import
import logging
from fastapi import FastAPI, HTTPException, Request, Depends, status
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import OAuth2PasswordRequestForm
import asyncio
import json
import requests
from sqlalchemy.orm import Session
from sqlalchemy import text, desc
from pydantic import BaseModel, EmailStr

# --- Logging Import ---
from api.logging_config import setup_logging
from api.sheets_logger import log_event

# --- DB Imports ---
from database.session import SessionLocal, init_db, engine
from database.models import Job as DBJob, User as DBUser # Use aliases to avoid name conflicts

# --- Auth Imports ---
from api import auth  # Our new auth module

# --- App Imports ---
from api.models import (
    ResearchRequest, JobSubmissionResponse, JobStatusResponse, ResearchResult, ExtractedData,
    RAGQueryRequest, RAGQueryResponse, RAGCollectionInfo, JobHistoryResponse
)
from src.config import assert_all_env, assert_rag_env
from src.rag_uploader import query_rag_collection

# +++ Import the Celery task +++
from src.tasks import run_research_pipeline_task

# +++ NEW: Pydantic models for auth +++
class UserCreate(BaseModel):
    email: EmailStr
    password: str

class UserPublic(BaseModel):
    id: str
    email: EmailStr
    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

# --- App Setup ---
app = FastAPI(
    title="Market Research Automation API",
    description="An API to run an automated market research pipeline.",
    version="1.0.0"
)

# This is the crucial part. It allows your frontend to talk to the backend.
# We will get the production URL from an environment variable.
FRONTEND_URL = os.getenv("FRONTEND_URL", "http://localhost:5173") 

origins = [
    "http://localhost:5173", # Default Vite dev server port
    "http://localhost:3000", # Common React dev server port
    "http://localhost:5174", # Another possible Vite port
    FRONTEND_URL, # Add your production URL here
]

# Remove any duplicates if FRONTEND_URL is a localhost one
origins = list(set(origins)) 
logging.info(f"Allowing origins: {origins}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"], # Allows all methods (GET, POST, etc.)
    allow_headers=["*"], # Allows all headers
)

# --- Database Initialization and Logging on Startup ---
@app.on_event("startup")
def on_startup():
    setup_logging()  # Set up logging first
    init_db()

# --- Dependency to get a DB session ---
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# +++ NEW: Auth Endpoints +++
@app.post("/api/auth/signup", response_model=UserPublic)
def signup(user_in: UserCreate, db: Session = Depends(get_db)):
    db_user = db.query(DBUser).filter(DBUser.email == user_in.email).first()
    if db_user:
        raise HTTPException(status_code=400, detail="Email already registered")
    
    hashed_password = auth.get_password_hash(user_in.password)
    new_user = DBUser(
        id=str(uuid.uuid4()),
        email=user_in.email, 
        hashed_password=hashed_password
    )
    db.add(new_user)
    db.commit()
    db.refresh(new_user)

    # +++ Add Logging +++
    log_event("user_signup", user_id=new_user.id, details={"email": new_user.email})

    return new_user

@app.post("/api/auth/token", response_model=Token)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    user = db.query(DBUser).filter(DBUser.email == form_data.username).first()
    if not user or not auth.verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect email or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # +++ Add Logging +++
    log_event("user_login", user_id=user.id)
    
    access_token = auth.create_access_token(data={"sub": user.id})
    return {"access_token": access_token, "token_type": "bearer"}

# --- Background Task (Moved to Celery) ---
# The run_and_store_results function has been moved to src/tasks.py as a Celery task


def _dict_to_extracted_model(raw_dict: dict) -> ExtractedData:
    padded = {k: raw_dict.get(k, []) for k in ["News", "Patents", "Conference", "Legalnews", "Other"]}
    return ExtractedData(**padded)


# --- API Endpoints (Now DB-aware) ---

@app.post("/api/research", response_model=JobSubmissionResponse, status_code=202)
async def create_research_job(
    request: Request,
    research_request: ResearchRequest,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(auth.get_current_user) # <-- PROTECT THE ROUTE
):
    assert_all_env()
    if research_request.upload_to_rag:
        try:
            assert_rag_env()
        except ValueError as e:
            raise HTTPException(status_code=400, detail=f"Cannot process RAG upload: {e}")

    job_id = str(uuid.uuid4())
    base_url = str(request.base_url)

    # Create the job record in the database
    new_job = DBJob(
        id=job_id,
        status="pending",
        original_query=research_request.query,
        upload_to_rag=research_request.upload_to_rag,
        rag_status="pending" if research_request.upload_to_rag else None,
        user_id=current_user.id  # <-- LINK THE JOB TO THE USER
    )
    db.add(new_job)
    db.commit()
    db.refresh(new_job)

    # +++ Add Logging +++
    log_event(
        "job_created", 
        user_id=current_user.id, 
        job_id=new_job.id, 
        query=new_job.original_query,
        details={"upload_to_rag": new_job.upload_to_rag}
    )

    # --- THIS IS THE KEY CHANGE ---
    # Instead of using BackgroundTasks, we send the job to the Celery queue.
    # The .delay() method is a shortcut to send a task message.
    run_research_pipeline_task.delay(
        job_id=job_id,
        query=research_request.query,
        should_upload_to_rag=research_request.upload_to_rag
    )
    logging.info(f"Dispatched job {job_id} to Celery worker.")

    return {
        "job_id": job_id,
        "status": "pending",
        "status_url": f"{base_url}api/research/status/{job_id}",
        "result_url": f"{base_url}api/research/result/{job_id}"
    }


@app.get("/api/research/status/{job_id}", response_model=JobStatusResponse)
async def get_research_status(job_id: str, db: Session = Depends(get_db), current_user: DBUser = Depends(auth.get_current_user)):
    job = db.query(DBJob).filter(DBJob.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    
    # Check if the job belongs to the current user
    if job.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied: Job belongs to another user")

    # --- MODIFIED: Construct a more detailed message and response ---
    message = f"Job status is {job.status}"
    if job.status == 'running' and job.job_stage:
        message = f"Executing stage: {job.job_stage.replace('_', ' ').title()}"
    
    if job.upload_to_rag:
        rag_status = job.rag_status or 'unknown'
        if rag_status == 'uploaded':
            message += f". RAG upload successful (Collection: {job.rag_collection_name or 'unknown'})"
        elif rag_status == 'failed':
            message += f". RAG upload failed: {job.rag_error or 'Unknown RAG error'}"
        else:
             message += f". RAG status: {rag_status}"
    
    if job.status == 'failed':
        error_msg = job.result.get('error', 'Unknown error') if job.result else 'Unknown error'
        message = f"Job failed. Error: {error_msg}"

    return {
        "job_id": job_id,
        "status": job.status,
        "message": message,
        "stage": job.job_stage,
        "progress": job.job_progress,
        # --- NEW: Return the logs array ---
        "logs": job.logs[-10:] if job.logs else [] # Return last 10 logs
    }


@app.get("/api/research/result/{job_id}", response_model=ResearchResult)
async def get_research_result(job_id: str, current_user: DBUser = Depends(auth.get_current_user)):
    """
    🔥 FIXED: Result endpoint with proper RAG info
    """
    db = SessionLocal()
    try:
        job = db.query(DBJob).filter(DBJob.id == job_id).first()
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        # Check if the job belongs to the current user
        if job.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Access denied: Job belongs to another user")
        
        if job.status == 'pending' or job.status == 'running':
            raise HTTPException(status_code=202, detail="Job still in progress")
        
        if job.status == 'failed':
            error_msg = job.result.get("error", "Unknown error") if job.result else "Unknown error"
            raise HTTPException(status_code=500, detail=f"Job failed: {error_msg}")
        
        if not job.result:
            raise HTTPException(status_code=500, detail="Job completed but no result found")
        
        # 🔥 CRITICAL: Build metadata with RAG info
        enhanced_metadata = job.result.get("metadata", {}).copy()
        
        if job.upload_to_rag:
            enhanced_metadata['ragInfo'] = {
                'upload_requested': True,
                'rag_status': job.rag_status or 'pending_upload',
                'collection_name': job.rag_collection_name,
                'rag_error': job.rag_error,
                'can_query': job.rag_status == 'uploaded'
            }
            logging.info(f"Job {job_id}: RAG Info - Status: {job.rag_status}, Can Query: {job.rag_status == 'uploaded'}")
        else:
            enhanced_metadata['ragInfo'] = {
                'upload_requested': False,
                'rag_status': 'not_requested',
                'can_query': False
            }
        
        response = ResearchResult(
            job_id=job.id,
            status='completed',
            original_query=job.result.get("original_query"),
            final_report_markdown=job.result.get("final_report_markdown"),
            extracted_data=_dict_to_extracted_model(job.result.get("extracted_data", {})),
            metadata=enhanced_metadata
        )
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"Error retrieving job result {job_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve job result: {str(e)}")
    finally:
        db.close()


# +++ NEW: Endpoint to get user's job history +++
@app.get("/api/research/history", response_model=JobHistoryResponse)
async def get_user_research_history(
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(auth.get_current_user)
):
    """
    Retrieves a list of all research jobs submitted by the current user.
    """
    # Query the database for jobs belonging to the current user,
    # ordering by the creation date in descending order (newest first).
    jobs = (
        db.query(DBJob)
        .filter(DBJob.user_id == current_user.id)
        .order_by(desc(DBJob.created_at))
        .all()
    )
    
    if not jobs:
        return {"jobs": []}  # Return an empty list if no jobs are found
        
    return {"jobs": jobs}


@app.post("/api/rag/query", response_model=RAGQueryResponse)
async def ask_rag_collection(query_request: RAGQueryRequest, db: Session = Depends(get_db), current_user: DBUser = Depends(auth.get_current_user)):
    try:
        assert_rag_env()

        # Find the job associated with the collection to get/update chat context
        job = db.query(DBJob).filter(DBJob.rag_collection_name == query_request.collection_name).first()
        if not job:
            raise HTTPException(status_code=404, detail=f"Collection '{query_request.collection_name}' not associated with any known job.")
        
        # Check if the job belongs to the current user
        if job.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Access denied: Collection belongs to another user")

        logging.info(f"Processing RAG query for collection: {query_request.collection_name}")
        logging.info(f"Question: {query_request.question}")

        # Pass the current context from the DB
        answer_payload = query_rag_collection(
            collection_name=query_request.collection_name,
            question=query_request.question,
            current_chat_context=job.rag_chat_context # Pass current context
        )

        logging.info(f"RAG API returned: {answer_payload}")

        # Update the chat context in the DB
        if answer_payload and 'chat_context' in answer_payload:
            new_fragment = answer_payload['chat_context']
            separator = " " if job.rag_chat_context else ""
            job.rag_chat_context += separator + new_fragment
            db.commit()

        # Ensure we're returning the correct structure
        response = RAGQueryResponse(
            collection_name=query_request.collection_name,
            question=query_request.question,
            answer=answer_payload
        )
        
        logging.info(f"Returning response: {response.dict()}")
        return response
        
    except ValueError as e:
        logging.error(f"RAG configuration error: {str(e)}")
        raise HTTPException(status_code=400, detail=f"RAG system not properly configured: {str(e)}")
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if e.response else 500
        error_detail = e.response.text if e.response else str(e)
        logging.error(f"RAG API HTTP error: {status_code} - {error_detail}")
        raise HTTPException(status_code=status_code, detail=f"Error from RAG API: {error_detail}")
    except Exception as e:
        logging.error(f"Unexpected error in RAG query: {str(e)}", exc_info=True)
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"An internal error occurred: {str(e)}")


@app.get("/api/research/{job_id}/rag", response_model=RAGCollectionInfo)
async def get_job_rag_info(job_id: str, db: Session = Depends(get_db), current_user: DBUser = Depends(auth.get_current_user)):
    job = db.query(DBJob).filter(DBJob.id == job_id).first()
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    
    # Check if the job belongs to the current user
    if job.user_id != current_user.id:
        raise HTTPException(status_code=403, detail="Access denied: Job belongs to another user")
    
    if not job.upload_to_rag:
        raise HTTPException(status_code=400, detail="RAG upload was not requested for this job")
    
    return RAGCollectionInfo(
        job_id=job_id,
        rag_status=job.rag_status or 'unknown',
        collection_name=job.rag_collection_name,
        rag_error=job.rag_error,
        can_query=(job.rag_status == 'uploaded' and job.rag_collection_name is not None)
    )


async def job_update_generator(job_id: str):
    """
    Yields real-time updates for a given job as Server-Sent Events.
    """
    db = SessionLocal()
    completion_sent = False
    
    try:
        while True:
            job = db.query(DBJob).filter(DBJob.id == job_id).first()
            if not job:
                break

            # Send status updates
            status_data = {
                'status': job.status, 
                'stage': job.job_stage, 
                'progress': job.job_progress
            }
            yield f"event: status\ndata: {json.dumps(status_data)}\n\n"

            # When job is completed, send result and close
            if job.status == 'completed' and job.result and not completion_sent:
                completion_sent = True
                
                # Send the full result
                final_payload = {
                    "job_id": job.id,
                    "status": 'completed',
                    "original_query": job.result.get("original_query"),
                    "final_report_markdown": job.result.get("final_report_markdown"),
                    "extracted_data": _dict_to_extracted_model(job.result.get("extracted_data", {})).dict(),
                    "metadata": job.result.get("metadata", {})
                }
                yield f"event: result\ndata: {json.dumps(final_payload)}\n\n"
                
                # Close the connection
                yield f"event: close\ndata: Job finished\n\n"
                break

            if job.status == 'failed':
                yield f"event: close\ndata: Job failed\n\n"
                break

            await asyncio.sleep(2)
            
    finally:
        db.close()

# --- NEW: SSE Endpoint ---
@app.get("/api/research/stream/{job_id}")
async def stream_research_status(job_id: str, current_user: DBUser = Depends(auth.get_current_user)):
    # Verify the job belongs to the current user before streaming
    db = SessionLocal()
    try:
        job = db.query(DBJob).filter(DBJob.id == job_id).first()
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        # Check if the job belongs to the current user
        if job.user_id != current_user.id:
            raise HTTPException(status_code=403, detail="Access denied: Job belongs to another user")
    finally:
        db.close()
    
    return StreamingResponse(job_update_generator(job_id), media_type="text/event-stream")


================================================
File: api/sheets_logger.py
================================================
import requests
import os
import logging
import threading

# Get the URL from environment variables
SHEETS_WEB_APP_URL = os.getenv("SHEETS_WEB_APP_URL")

def log_event(event: str, user_id: str = None, job_id: str = None, query: str = None, details: dict = None):
    """
    Sends a log event to a Google Sheets Web App.
    Runs in a non-blocking background thread.
    """
    if not SHEETS_WEB_APP_URL:
        # Log a warning if the feature is used but not configured
        logging.warning("SHEETS_WEB_APP_URL is not set. Skipping Google Sheets logging.")
        return

    payload = {
        "event": event,
        "userId": user_id,
        "jobId": job_id,
        "query": query,
        "details": details or {}
    }
    
    def send_request():
        """The function that will run in the background thread."""
        try:
            response = requests.post(SHEETS_WEB_APP_URL, json=payload, timeout=10)
            response.raise_for_status()
            logging.info(f"Successfully logged event '{event}' to Google Sheets.")
        except requests.exceptions.RequestException as e:
            logging.error(f"Failed to log event to Google Sheets: {e}")

    # Run the request in a daemon thread so it doesn't block the API response
    thread = threading.Thread(target=send_request, daemon=True)
    thread.start() 



================================================
File: database/__init__.py
================================================
# database/__init__.py
# This file makes the database directory a Python package 


================================================
File: database/models.py
================================================
# database/models.py
from sqlalchemy import Column, String, JSON, Boolean, Text, Integer, DateTime, ForeignKey
from sqlalchemy.orm import relationship
from datetime import datetime
from database.session import Base

# +++ NEW User Model +++
class User(Base):
    __tablename__ = "users"

    id = Column(String, primary_key=True, index=True)
    email = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

    # This creates the link back to the Job model
    jobs = relationship("Job", back_populates="owner")


# --- MODIFIED Job Model ---
class Job(Base):
    __tablename__ = "jobs"

    # Core job details
    id = Column(String, primary_key=True, index=True)
    status = Column(String, index=True, default="pending")
    original_query = Column(Text)
    result = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # RAG-specific details
    upload_to_rag = Column(Boolean, default=False)
    rag_status = Column(String, nullable=True)
    rag_collection_name = Column(String, nullable=True)
    rag_error = Column(String, nullable=True)
    
    # To store the conversational history for RAG
    rag_chat_context = Column(Text, default="") 

    # --- NEW: Add these two columns for structured status tracking ---
    job_stage = Column(String, nullable=True, default="pending")
    job_progress = Column(Integer, nullable=True, default=0)
    
    # --- NEW: Add this column to store live logs ---
    logs = Column(JSON, default=[]) 

    # +++ NEW: Link to the User model +++
    user_id = Column(String, ForeignKey("users.id"))
    owner = relationship("User", back_populates="jobs") 


================================================
File: database/session.py
================================================
# File: database/session.py (Corrected Version)

import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base

# Get the database URL from environment variables, defaulting to SQLite for local dev
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./jobs.db")

# Hide sensitive parts of the URL for logging
if "@" in DATABASE_URL:
    log_db_url = DATABASE_URL.split('@')[-1]
else:
    log_db_url = DATABASE_URL

print(f"💽 Connecting to database at: {log_db_url}")

# --- THIS IS THE CORRECTED LOGIC ---
# Prepare keyword arguments for create_engine
engine_args = {}

# Only add the connect_args if we are using SQLite.
if DATABASE_URL.startswith("sqlite"):
    engine_args['connect_args'] = {"check_same_thread": False}

# Create the engine, unpacking the arguments dictionary.
# If it's not SQLite, the dictionary will be empty and no extra args are passed.
engine = create_engine(DATABASE_URL, **engine_args)


# --- The rest of the file is unchanged ---
# Each instance of SessionLocal will be a database session.
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Base class for our declarative models
Base = declarative_base()

# Function to create all tables in the database
def init_db():
    print("Initializing database and creating tables (if they don't exist)...")
    # This will create tables for all models that inherit from Base
    from database.models import Job, User  # Import models here
    Base.metadata.create_all(bind=engine, checkfirst=True)
    print("Database initialized.")



================================================
File: src/__init__.py
================================================



================================================
File: src/config.py
================================================
import os
from dotenv import load_dotenv

# Load environment variables from the .env file in the project root
# This line looks for the .env file in the parent directory of src/
dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path=dotenv_path)

# --- API Keys ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")

# --- RAG API Config ---
RAG_API_BASE_URL = os.getenv("RAG_API_BASE_URL")
RAG_API_TOKEN = os.getenv("RAG_API_TOKEN")
RAG_API_ORG_ID = os.getenv("RAG_API_ORG_ID")
RAG_API_USER_TYPE = os.getenv("RAG_API_USER_TYPE")

# --- Startup Banner ---
def _mask_key(key_value: str | None) -> str:
    """Mask API key for display, showing only first 4 and last 4 characters."""
    if not key_value:
        return "❌ NOT SET"
    if len(key_value) <= 8:
        return "✅ SET (short)"
    return f"✅ SET ({key_value[:4]}...{key_value[-4:]})"

print("\n" + "="*60)
print("🔧 MARKET INTELLIGENCE AGENT - Environment Status")
print("="*60)
print(f"GEMINI_API_KEY:  {_mask_key(GEMINI_API_KEY)}")
print(f"GOOGLE_API_KEY:  {_mask_key(GOOGLE_API_KEY)}")
print(f"GOOGLE_CSE_ID:   {_mask_key(GOOGLE_CSE_ID)}")
print("-" * 60)
print("🔌 RAG Uploader Status")
print(f"RAG_API_BASE_URL:    {RAG_API_BASE_URL or '❌ NOT SET'}")
print(f"RAG_API_TOKEN: {_mask_key(RAG_API_TOKEN)}")
print(f"RAG_API_ORG_ID:      {RAG_API_ORG_ID or '❌ NOT SET'}")
print("="*60 + "\n")

# --- Validation ---
def assert_all_env():
    if not all([GEMINI_API_KEY, GOOGLE_API_KEY, GOOGLE_CSE_ID]):
        raise ValueError("Missing env vars: set GEMINI_API_KEY, GOOGLE_API_KEY, GOOGLE_CSE_ID")

def assert_rag_env():
    """Checks if all required RAG environment variables are set."""
    if not all([RAG_API_BASE_URL, RAG_API_TOKEN, RAG_API_ORG_ID, RAG_API_USER_TYPE]):
        raise ValueError("Missing env vars for RAG Uploader: set RAG_API_BASE_URL, RAG_API_TOKEN, RAG_API_ORG_ID, and RAG_API_USER_TYPE")

# You can add other configurations here later
# For example:
# LLM_PLANNER_MODEL = "gemini-1.5-pro-latest"
# LLM_SYNTHESIZER_MODEL = "gemini-1.5-flash-latest"


================================================
File: src/constants.py
================================================
# -------------------------------------------------
#  Global throttling & limit switches for the pipeline
# -------------------------------------------------
MAX_SEARCH_RESULTS   = 4        # items per Google-CSE query
MAX_SEARCH_WORKERS   = 18    # parallel threads for CSE calls

MAX_GENERAL_FOR_REPORT = 18  # cap "General" URLs that feed the executive report

MAX_PER_BUCKET_EXTRACT = 6   # News / Patents / Conf / Legalnews
EXTRACT_BATCH_SIZE     = 18  # 2 × batches → 18 URLs each
MAX_GEMINI_PARALLEL    = 30  # concurrent Gemini requests in extractor

# ---- NEW ----  Global “freshness” policy -------------------------
RECENT_YEARS = 2             # only keep items from the last N calendar years



================================================
File: src/main.py
================================================
# src/main.py (Refactored)
import json
import asyncio
import time
import logging
from concurrent.futures import ThreadPoolExecutor
import os
from typing import Callable, Any  # <-- Import Callable and Any

# Keep all your existing phase imports
from src.phase1_planner import generate_search_queries
from src.phase2_searcher import execute_cse_searches
from src.phase3_intermediate_synthesizer import synthesize_all_intermediate_reports
from src.phase5_final_synthesizer import synthesize_final_report
from src.phase4_extractor import run_structured_extraction
from src.constants import MAX_SEARCH_WORKERS, MAX_GENERAL_FOR_REPORT, MAX_PER_BUCKET_EXTRACT
from src.config import assert_all_env

# Configuration: adjust parallelism limits
MAX_BATCH_WORKERS = 6

async def execute_research_pipeline(
    user_query: str, 
    update_status: Callable
) -> dict:
    """
    OPTIMIZED: Pipeline with better parallelization.
    """
    assert_all_env()
    start_time = time.perf_counter()
    logging.info(f"--- Starting Optimized Pipeline for query: '{user_query[:50]}...' ---")
    
    # Phase 1 & 2 unchanged...
    await update_status(stage="planning", progress=10, message="Analyzing request and planning search strategies...")
    search_queries = generate_search_queries(user_query)
    if not search_queries:
        raise ValueError("Pipeline Error: No search queries were generated.")
    
    total_queries = sum(len(queries) for queries in search_queries.values())
    logging.info(f"-> Phase 1 Complete: {total_queries} queries generated.")
    
    await update_status(stage="searching", progress=25, message=f"Scouring {total_queries} web sources...")
    tagged_urls = await execute_cse_searches(search_queries)
    if not tagged_urls:
        raise ValueError("Pipeline Error: No URLs were collected from search.")
    
    logging.info(f"-> Phase 2 Complete: {len(tagged_urls)} URLs collected.")
    
    # Prepare URL buckets
    bucketed: dict[str, list[str]] = {}
    for url, bucket in tagged_urls:
        bucketed.setdefault(bucket, []).append(url)

    general_urls = bucketed.get("General", [])[:MAX_GENERAL_FOR_REPORT]
    news_urls = bucketed.get("News", [])[:MAX_PER_BUCKET_EXTRACT]
    patents_urls = bucketed.get("Patents", [])[:MAX_PER_BUCKET_EXTRACT]
    conf_urls = bucketed.get("Conference", [])[:MAX_PER_BUCKET_EXTRACT]
    legalnews_urls = bucketed.get("Legalnews", [])[:MAX_PER_BUCKET_EXTRACT]

    report_urls = general_urls
    extract_urls = news_urls + patents_urls + conf_urls + legalnews_urls
    url2tag = {u: "News" for u in news_urls} | \
              {u: "Patents" for u in patents_urls} | \
              {u: "Conference" for u in conf_urls} | \
              {u: "Legalnews" for u in legalnews_urls}
    
    logging.info(f"-> URL Distribution: Report={len(report_urls)}, Extract={len(extract_urls)}")

    # 🔥 CRITICAL CHANGE: Start extraction and synthesis in parallel
    await update_status(stage="synthesizing", progress=50, message="Starting parallel analysis...")
    
    # Start extraction immediately
    extraction_task = asyncio.create_task(
        run_structured_extraction(extract_urls, user_query, url2tag)
    )
    
    # Start intermediate synthesis in parallel
    url_batches = [report_urls[i:i+15] for i in range(0, len(report_urls), 15)]
    intermediate_reports_task = asyncio.get_event_loop().run_in_executor(
        ThreadPoolExecutor(1),
        synthesize_all_intermediate_reports,
        user_query,
        url_batches,
        "reports/intermediate_reports",
        True,
        MAX_BATCH_WORKERS
    )
    
    # Wait for both to complete
    logging.info("-> Running extraction and synthesis in parallel...")
    intermediate_reports, extraction_payload = await asyncio.gather(
        intermediate_reports_task,
        extraction_task
    )
    
    logging.info(f"-> Parallel processing complete.")
    
    # Final synthesis
    await update_status(stage="compiling", progress=85, message="Generating final report...")
    final_report_path = await asyncio.get_event_loop().run_in_executor(
        ThreadPoolExecutor(1),
        synthesize_final_report,
        user_query,
        intermediate_reports,
        report_urls
    )
    
    # Read final report
    try:
        with open(final_report_path, 'r', encoding='utf-8') as f:
            final_report_content = f.read()
    except FileNotFoundError:
        final_report_content = "Error: Final report could not be generated or found."

    elapsed = time.perf_counter() - start_time
    logging.info(f"--- Optimized Pipeline complete in {elapsed:.2f} seconds ---")

    return {
        "original_query": user_query,
        "final_report_markdown": final_report_content,
        "intermediate_reports": intermediate_reports,
        "metadata": extraction_payload["metadata"],
        "extracted_data": extraction_payload["extracted_data"],
    }


# This block is for standalone testing if you ever need it
if __name__ == "__main__":
    USER_QUERY = """Show me the latest innovations in Weatherability of Decorative Coatings.
What trends are emerging in the Sustainability of industrial coatings in 2025?
Find recent conferences or Patents discussing Scuff-Resistance in coatings.

Search tags/topics - Product, coating, architectural or similar.

Datasources/URLs (https://www.paint.org/ , https://www.coatingsworld.com/ , https://www.pcimag.com/ )"""
    
    async def main():
        # Dummy callback for standalone testing
        async def dummy_callback(stage: str, progress: int, message: str):
            logging.info(f"[{progress}%] {stage}: {message}")
        
        try:
            results = await execute_research_pipeline(USER_QUERY, dummy_callback)
            logging.info("\n\n--- PIPELINE RESULT ---")
            logging.info("\n## FINAL REPORT (Snippet) ##")
            logging.info(results['final_report_markdown'][:500] + "...")
            logging.info("\n## EXTRACTED DATA (Summary) ##")
            logging.info(json.dumps(results['metadata']['extraction_summary'], indent=2))
        except Exception as e:
            logging.error(f"An error occurred: {e}")

    asyncio.run(main())


================================================
File: src/phase1_planner.py
================================================
# src/phase1_planner.py
import base64
import os
import logging
from datetime import datetime
from google import genai
from google.genai import types
import json
from src import config # Our configuration loader
from src import constants

def generate_search_queries(user_input: str) -> dict[str, list[str]]:
    """
    Uses the Gemini API to analyze user input and generate a dictionary of
    targeted Google CSE search queries organized by category using the correct SDK syntax.

    Args:
        user_input: The full text of the user's request.

    Returns:
        A dictionary with keys News, Patents, Conference, Legalnews, General,
        where each value is a list of search query strings.
        Returns empty lists for each category if generation fails.
    """
    logging.info("Phase 1: Generating search queries with Gemini...")

    try:
        # 1. Instantiate the client using the API key from our config
        client = genai.Client(api_key=config.GEMINI_API_KEY)

        # Get current date for recency context
        current_date = datetime.now()
        current_year = current_date.year
        current_month = current_date.strftime("%B")
        
        # 2. Define the enhanced prompt
        prompt = f"""
You are a world-class market intelligence strategist for a top-tier global chemical company, with deep expertise in the decorative and industrial coatings sectors. Your mission is to formulate precise search queries to uncover actionable intelligence for our business and R&D teams.

CURRENT DATE CONTEXT:
Today is {current_month} {current_date.day}, {current_year}. The intelligence must be current. Prioritize information from {current_year} and the last {constants.RECENT_YEARS} years ({current_year - constants.RECENT_YEARS + 1}-{current_year}).

USER'S STRATEGIC OBJECTIVE:
{user_input}

TASK:
Generate a JSON dictionary of Google Custom Search Engine (CSE) queries. These queries will be run against a curated list of trusted industry sources. Your queries must be designed to uncover breakthrough technologies, competitive shifts, regulatory changes, and emerging market needs.

QUERY BUCKETS:
Group your queries into the following strategic buckets:
- "News": Corporate announcements, M&A activity, financial results, partnerships.
- "Patents": Patent filings, new intellectual property, and technical whitepapers. Focus on novel chemistries and formulations.
- "Conference": Key findings, presentations, and announcements from major industry events (e.g., American Coatings Show, European Coatings Show).
- "Legalnews": New environmental regulations (like VOC limits), chemical bans, and compliance standards.
- "General": Broader queries on technology trends, market analysis, and material science innovations.

MANDATORY RULES:
1.  **Source Specificity:** Every query MUST use a `site:` operator to target a single domain from the user's provided list.
2.  **Strategic Keywords:** Go beyond simple topics. Combine technologies (e.g., "polyurethane dispersion," "silane-modified polymers," "hydrophobic additives") with performance outcomes (e.g., "scuff resistance," "weatherability," "self-healing," "oleophobic") and business context (e.g., "market trend," "supply chain," "sustainability report").
3.  **Recency Bias:** Embed recency terms in the queries: "{current_year}," "{current_year + 1} forecast," "latest," "emerging," "new."
4.  **Actionable Intelligence Focus:** Formulate queries to find data. Think like a strategist: What would you search for to find competitor weaknesses or new market opportunities?
5.  **Output Format:** Return ONLY the JSON object. Do not include any explanatory text, markdown formatting, or apologies.

EXAMPLE OF A HIGH-QUALITY QUERY:
"latest developments in scuff-resistant architectural coatings {current_year} site:coatingsworld.com"
"sustainability initiatives AkzoNobel coatings {current_year + 1} report site:paint.org"

JSON OUTPUT STRUCTURE:
{{
  "News":        [ "...", "..." ],
  "Patents":     [ "...", "..." ],
  "Conference":  [ "...", "..." ],
  "Legalnews":   [ "...", "..." ],
  "General":     [ "...", "..." ]
}}
"""
        
        # 3. Structure the request using types.Content and types.Part
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part(text=prompt),
                ],
            ),
        ]
        
        # 4. Create the generation configuration object
        # This is the correct way to specify safety and response type.
        generate_content_config = types.GenerateContentConfig(
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ],
            response_mime_type="application/json",
        )

        # 5. Make the API call using the CORRECT method: client.models.generate_content
        model_name = "gemini-2.5-flash-preview-05-20"
        response = client.models.generate_content(
            model=f"models/{model_name}",
            contents=contents,
            config=generate_content_config,
        )
        
        # 6. Parse the JSON response
        raw = response.candidates[0].content.parts[0].text
        data = json.loads(raw)
        by_bucket = {k: data.get(k, []) for k in
                    ["News","Patents","Conference","Legalnews","General"]}

        # Clean the queries in each bucket to remove protocol and 'www' for the site: operator
        import re
        cleaned_buckets = {}
        total_queries = 0
        
        for bucket_name, queries in by_bucket.items():
            if isinstance(queries, list):
                cleaned_queries = []
                for q in queries:
                    # This regex finds the domain part and rebuilds the query
                    match = re.search(r"site:https?://(?:www\.)?([^/\s]+)", q)
                    if match:
                        domain = match.group(1)
                        # Replace the full url part with just the domain
                        cleaned_q = re.sub(r"site:https?://(?:www\.)?[^/\s]+", f"site:{domain}", q)
                        cleaned_queries.append(cleaned_q)
                    else:
                        cleaned_queries.append(q) # Append as-is if no match
                cleaned_buckets[bucket_name] = cleaned_queries
                total_queries += len(cleaned_queries)
            else:
                logging.warning(f"LLM returned '{bucket_name}' but it was not a list. Using empty list.")
                cleaned_buckets[bucket_name] = []

        logging.info(f"Successfully generated and cleaned {total_queries} search queries across {len(cleaned_buckets)} buckets.")
        return cleaned_buckets

    except json.JSONDecodeError as e:
        logging.error(f"Failed to parse JSON from the LLM response. Error: {e}")
        raw = None
        try:
            raw = response.candidates[0].content.parts[0].text
        except Exception:
            pass
        logging.error(f"----- LLM Raw Response -----\n{raw if raw is not None else 'No text in response'}\n--------------------------")
        return {k: [] for k in ["News","Patents","Conference","Legalnews","General"]}
    except Exception as e:
        logging.error(f"An unexpected error occurred during query generation: {e}")
        return {k: [] for k in ["News","Patents","Conference","Legalnews","General"]}


================================================
File: src/phase2_searcher.py
================================================
# src/phase2_searcher.py  (fully rewritten)

import asyncio, re, time, logging
import httpx
from datetime import date
from src import config, constants

CSE_ENDPOINT = "https://customsearch.googleapis.com/customsearch/v1"

async def _single_cse(client: httpx.AsyncClient, query: str, bucket: str,
                      num_results: int, idx: int) -> list[tuple[str, str]]:
    """Fire one CSE request, return (url, bucket) pairs."""
    year_from = date.today().year - constants.RECENT_YEARS
    params = {
        "q": query,
        "cx": config.GOOGLE_CSE_ID,
        "key": config.GOOGLE_API_KEY,
        "num": num_results,
        "sort": f"date:r:{year_from}0101:{date.today():%Y%m%d}",
    }
    try:
        r = await client.get(CSE_ENDPOINT, params=params, timeout=20)
        r.raise_for_status()
        items = r.json().get("items", [])
        return [(it["link"], bucket) for it in items]
    except httpx.HTTPStatusError as e:
        # retry once without the sort parameter on 400
        if e.response.status_code == 400 and "sort" in params:
            params.pop("sort", None)
            r = await client.get(CSE_ENDPOINT, params=params, timeout=20)
            r.raise_for_status()
            items = r.json().get("items", [])
            return [(it["link"], bucket) for it in items]
        logging.warning(f"CSE error {e.response.status_code} for query #{idx}: {query[:60]}")
    except Exception as e:
        logging.warning(f"{e} on query #{idx}")
    return []

async def execute_cse_searches(queries_by_type: dict[str, list[str]],
                               num_results: int = constants.MAX_SEARCH_RESULTS,
                               max_concurrency: int = constants.MAX_SEARCH_WORKERS
                               ) -> list[tuple[str, str]]:
    """
    Fully asynchronous Google CSE runner.  No thread pools, HTTP/2, 1-RTT.
    Returns deduped (url, bucket) list.
    """
    flat: list[tuple[str, str]] = [
        (bucket, q) for bucket, lst in queries_by_type.items() for q in lst
    ]
    if not flat:
        return []

    t0 = time.perf_counter()
    tagset: set[tuple[str, str]] = set()

    limits = httpx.Limits(max_connections=max_concurrency, max_keepalive_connections=max_concurrency)
    async with httpx.AsyncClient(http2=True, limits=limits) as client:
        sem = asyncio.Semaphore(max_concurrency)

        async def _wrapped(i, bucket, query):
            async with sem:
                return await _single_cse(client, query, bucket, num_results, i)

        tasks = [
            asyncio.create_task(_wrapped(i, bucket, query))
            for i, (bucket, query) in enumerate(flat)
        ]
        for coro in asyncio.as_completed(tasks):
            for link, bucket in await coro:
                tagset.add((link, bucket))

    elapsed = time.perf_counter() - t0
    logging.info(f"Phase 2 – {len(tagset)} unique URLs in {elapsed:0.1f}s "
          f"({len(flat)} queries, {max_concurrency} concurrency)")
    return list(tagset)


================================================
File: src/phase3_intermediate_synthesizer.py
================================================
# src/phase3_intermediate_synthesizer.py

import os
import re
import datetime
import logging
from datetime import date
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Tuple
from google import genai
from google.genai import types
from src import config
from src import constants
import hashlib

def synthesize_intermediate_report(
    original_user_query: str,
    urls_batch: list[str],
    batch_index: int = 0,
    output_dir: str = "reports/intermediate_reports"
) -> str:
    """
    Generates a focused Markdown sub-report for a given URL batch using Gemini's UrlContext.
    """
    if not urls_batch:
        logging.info(f"    - Batch {batch_index}: No URLs provided — skipping.")
        return ""

    logging.info(f"    - Batch {batch_index}: Synthesizing {len(urls_batch)} URLs...")

    client = genai.Client(api_key=config.GEMINI_API_KEY)

    # Get current date for context
    current_date = date.today()
    current_year = current_date.year
    target_years = f"{current_year - constants.RECENT_YEARS + 1}-{current_year}"

    system_instruction = (
        f"You are a senior market intelligence analyst at a leading chemical company like PPG or Sherwin-Williams. "
        f"Your audience is an R&D Director or a Business Unit Manager. They are technically savvy and time-poor. "
        f"They need to know the 'so what?' of the information presented. Today is {current_date.strftime('%B %d, %Y')}.\n\n"
        
        "**MISSION: Distill raw intelligence into a concise, data-driven sub-report.**\n"
        f"Work exclusively from the provided URLs. Focus on information from the last {constants.RECENT_YEARS} years ({target_years}).\n\n"
        
        "**REQUIRED REPORT STRUCTURE:**\n"
        "Synthesize your findings into the following Markdown structure. For each point, focus on its significance.\n\n"
        
        "1. **Top-Line Summary** (2-3 crucial sentences)\n"
        "   - What is the single most important takeaway from these sources for our business?\n\n"
        
        "2. **Key Technical & Performance Data**\n"
        "   - Extract specific performance metrics, test results (e.g., ASTM standards), and chemical formulations.\n"
        "   - Note any quantitative improvements mentioned (e.g., '30% increase in scuff resistance').\n"
        "   - **Implication:** Does this represent a threat to our existing products or an opportunity for innovation?\n\n"
        
        "3. **Market & Competitive Intelligence**\n"
        "   - Identify market size/growth figures, and any mention of competitor activities (e.g., product launches, plant openings by AkzoNobel, BASF, etc.).\n"
        "   - **Implication:** How does this shift the competitive landscape?\n\n"
        
        "4. **Sustainability & Regulatory Impact**\n"
        "   - Pinpoint new regulations (e.g., VOC limits) or sustainability initiatives (e.g., bio-based content, circular economy).\n"
        "   - **Implication:** What are the product development or compliance consequences for us?\n\n"
        
        "**QUALITY & TONE STANDARDS:**\n"
        "• **Lead with Data:** Prioritize numbers, percentages, and dates. If a source quantifies something, you must include it.\n"
        "• **Objective & Analytical Tone:** Avoid marketing fluff. Be direct and factual.\n"
        "• **Note Contradictions:** If sources conflict, state the discrepancy clearly.\n"
        "• **Cite Your Work:** Use inline numeric citations `[1]`, `[2]` corresponding to the URL list. Do not add a final reference list.\n\n"
        
        "**OUTPUT:** A dense, actionable Markdown report that empowers a manager to make an informed decision."
    )

    # Combine system instruction with user instruction since Gemini only accepts "user" and "model" roles
    combined_instruction = (
        f"{system_instruction}\n\n"
        f"**RESEARCH QUERY:** {original_user_query}\n\n"
        f"**SOURCE URLS (Batch {batch_index}):**\n" +
        "\n".join(f"{i+1}. {url}" for i, url in enumerate(urls_batch)) +
        "\n\n**TASK:** Analyze these sources and generate a comprehensive market intelligence sub-report following the structure above."
    )

    contents = [
        types.Content(role="user", parts=[types.Part(text=combined_instruction)])
    ]

    tools = [types.Tool(url_context=types.UrlContext())]
    config_obj = types.GenerateContentConfig(
        tools=tools,
        safety_settings=[
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_MEDIUM_AND_ABOVE"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
        ],
        response_modalities=["TEXT"],
    )

    report_fragments = []
    try:
        stream = client.models.generate_content_stream(
            model="models/gemini-2.5-flash-preview-05-20",
            contents=contents,
            config=config_obj,
        )
        for chunk in stream:
            report_fragments.append(chunk.text)
        intermediate_md = "".join(report_fragments).strip()

        # Save sub-report
        os.makedirs(output_dir, exist_ok=True)
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = hashlib.sha1(original_user_query.encode()).hexdigest()[:16]
        path = os.path.join(output_dir, f"{ts}_batch{batch_index}_{safe_name}.md")
        with open(path, "w", encoding="utf-8") as f:
            f.write(f"## Batch {batch_index} Intermediate Report\n\n")
            f.write(intermediate_md)
        logging.info(f"    - Batch {batch_index}: sub-report saved → {path}")
        return intermediate_md

    except Exception as e:
        err = f"## Batch {batch_index} – Error during synthesis:\n{e}"
        logging.error(f"    - {err}")
        return err


def synthesize_intermediate_reports_parallel(
    original_user_query: str,
    url_batches: List[List[str]],
    output_dir: str = "reports/intermediate_reports",
    max_workers: int = None
) -> List[Tuple[int, str]]:
    """
    Generates multiple intermediate reports in parallel for given URL batches using Gemini's UrlContext.
    
    Args:
        original_user_query: The original research query
        url_batches: List of URL batches to process in parallel
        output_dir: Directory to save intermediate reports
        max_workers: Maximum number of parallel workers (defaults to min(len(batches), 8))
    
    Returns:
        List of tuples containing (batch_index, report_content)
    """
    if not url_batches:
        logging.info("No URL batches provided for parallel processing.")
        return []

    # Calculate optimal number of workers
    if max_workers is None:
        max_workers = min(len(url_batches), 8)  # Cap at 8 to avoid overwhelming the API
    
    logging.info(f"Starting parallel synthesis of {len(url_batches)} batches with {max_workers} workers...")
    
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all batch processing tasks
        future_to_batch = {}
        for batch_index, urls_batch in enumerate(url_batches):
            if urls_batch:  # Only submit non-empty batches
                future = executor.submit(
                    synthesize_intermediate_report,
                    original_user_query,
                    urls_batch,
                    batch_index,
                    output_dir
                )
                future_to_batch[future] = batch_index
        
        # Collect results as they complete
        for future in as_completed(future_to_batch):
            batch_index = future_to_batch[future]
            try:
                report_content = future.result()
                results.append((batch_index, report_content))
                logging.info(f"✓ Completed batch {batch_index}")
            except Exception as e:
                error_msg = f"## Batch {batch_index} – Error during parallel synthesis:\n{e}"
                results.append((batch_index, error_msg))
                logging.error(f"✗ Failed batch {batch_index}: {e}")
    
    # Sort results by batch index to maintain order
    results.sort(key=lambda x: x[0])
    
    logging.info(f"✓ Parallel synthesis completed: {len(results)} batches processed")
    return results


def synthesize_all_intermediate_reports(
    original_user_query: str,
    url_batches: List[List[str]],
    output_dir: str = "reports/intermediate_reports",
    use_parallel: bool = True,
    max_workers: int = None
) -> List[str]:
    """
    Convenience function to synthesize all intermediate reports either in parallel or sequentially.
    
    Args:
        original_user_query: The original research query
        url_batches: List of URL batches to process
        output_dir: Directory to save intermediate reports
        use_parallel: Whether to use parallel processing (default: True)
        max_workers: Maximum number of parallel workers (only used if use_parallel=True)
    
    Returns:
        List of report contents in batch order
    """
    if use_parallel:
        results = synthesize_intermediate_reports_parallel(
            original_user_query, url_batches, output_dir, max_workers
        )
        # Extract just the report contents in order
        return [report_content for _, report_content in results]
    else:
        # Sequential processing (original behavior)
        reports = []
        for batch_index, urls_batch in enumerate(url_batches):
            report = synthesize_intermediate_report(
                original_user_query, urls_batch, batch_index, output_dir
            )
            reports.append(report)
        return reports



================================================
File: src/phase4_extractor.py
================================================
#!/usr/bin/env python3
import sys
import os
# Ensure project root is on PYTHONPATH for script execution
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

import asyncio
import json
import datetime
from datetime import date
import re
import time
import logging
from google import genai
from google.genai import types
from src import config
from src.constants import MAX_GEMINI_PARALLEL, EXTRACT_BATCH_SIZE
from src import constants
from dateutil import parser as dtparse
from operator import itemgetter

def _is_recent(date_str: str | None) -> bool:
    """
    True ⇢ item year >= (today - RECENT_YEARS)  • False otherwise
    """
    if not date_str:
        return False
    try:
        yr = dtparse.parse(date_str, fuzzy=True).year
        return yr >= date.today().year - constants.RECENT_YEARS
    except (dtparse.ParserError, TypeError):
        return False

# --- New: guarantee every category key exists  ------------------
EXPECTED_CATEGORIES = ["News", "Patents", "Conference", "Legalnews", "Other"]

def _pad_categories(cat_dict: dict) -> dict:
    """Return a dict that has every required category, even if empty."""
    return {k: list(cat_dict.get(k, [])) for k in EXPECTED_CATEGORIES}
# ----------------------------------------------------------------

def extract_data_from_single_url_sync(
    url: str,
    client: genai.Client
) -> list[dict]:
    """
    Uses Gemini to extract structured items (news, Patents, conferences, Legalnews) from a URL.
    Returns parsed list of item dicts.
    """
    logging.info(f"    - Extracting from: {url}")
    try:
        # Get current date for context
        current_date = date.today()
        current_year = current_date.year
        target_years = f"{current_year - constants.RECENT_YEARS + 1}-{current_year}"
        
        instruction = (
            f"You are a high-precision data extraction engine. Your sole function is to parse an online document and extract specific, structured information related to the coatings industry. You must be rigorous and discard any item that does not meet the criteria perfectly. Today's date is {current_date.strftime('%Y-%m-%d')}.\n\n"
            
            f"**SOURCE URL TO ANALYZE:** {url}\n\n"
            
            "**EXTRACTION TASK & CATEGORIES:**\n"
            "From the URL content, extract ONLY English-language items from the last {constants.RECENT_YEARS} years ({target_years}) that fit one of these exact categories:\n"
            "• **News**: Company announcements, market reports, financial updates.\n"
            "• **Patents**: Patent applications, grants, or detailed technical whitepapers on novel technology.\n"
            "• **Conference**: Announcements or summaries of industry events, webinars, or presentations.\n"
            "• **Legalnews**: Regulatory updates, new chemical standards, or legal cases relevant to coatings.\n\n"
            
            "**JSON OBJECT FIELD REQUIREMENTS (STRICT):**\n"
            "For each qualifying item, create a JSON object with these exact 5 fields:\n\n"
            
            "1. **`type`**: (String) Must be one of: 'News', 'Patents', 'Conference', 'Legalnews'. Classify based on the primary focus of the content.\n\n"
            
            "2. **`title`**: (String) The official title of the article or patent. If none, create a concise, descriptive title (5-15 words). MUST be relevant to coatings.\n\n"
            
            "3. **`summary`**: (String, 100-300 words) A self-contained, detailed summary. Must include key entities (companies, products), quantitative data (percentages, market values), and the core finding's significance to the coatings industry. Do not assume the user will read the source.\n\n"
            
            "4. **`date`**: (String or Null) The publication date in **YYYY-MM-DD** format. If only month/year are available, use the first day (e.g., '2024-05-01'). If the date cannot be reliably determined or is outside the {constants.RECENT_YEARS}-year window, this field MUST be `null`.\n\n"
            
            "5. **`source_url`**: (String) The exact URL provided for analysis: `{url}`\n\n"
            
            "**RIGOROUS QUALITY CONTROL:**\n"
            "• **Precision is Key:** If an item is ambiguous or its relevance to coatings is weak, **DO NOT** include it. It is better to return an empty array than incorrect data.\n"
            "• **No Duplicates:** If one article mentions two separate products, create two distinct JSON objects.\n"
            "• **Validate Dates:** Strictly enforce the recency filter. Exclude older content.\n"
            "• **Strict JSON:** The final output must be a single, valid JSON array `[...]`. No text before or after.\n\n"
            
            "**FINAL OUTPUT FORMAT:**\n"
            "Return a valid JSON array of objects. Return an empty array `[]` if no qualifying items are found."
        )

        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part(text=instruction)
                ],
            )
        ]
        tools = [types.Tool(url_context=types.UrlContext())]
        config_obj = types.GenerateContentConfig(
            tools=tools,
            response_modalities=["TEXT"],
            safety_settings=[
                types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
            ],
        )
        response = client.models.generate_content(
            model="gemini-2.5-flash-preview-05-20",
            contents=contents,
            config=config_obj,
        )
        
        if not response.candidates or not response.candidates[0].content or not response.candidates[0].content.parts:
            logging.warning(f"      → No content part in Gemini response for: {url}")
            return []
            
        text_output = response.candidates[0].content.parts[0].text.strip()
        # Extract the JSON array substring
        match = re.search(r"\[.*\]", text_output, re.S)
        if not match:
            logging.warning(f"      → No JSON array in response for: {url}\n        Response: {text_output[:100]}...")
            return []
        items = json.loads(match.group(0))
        return items if isinstance(items, list) else []

    except json.JSONDecodeError as e:
        logging.error(f"      → JSONDecodeError for URL {url}: {e}")
        return []
    except Exception as e:
        logging.error(f"      → Error processing {url}: {e}")
        return []

async def run_structured_extraction(
    urls: list[str],
    original_user_query: str,
    url2tag: dict[str,str],
    output_dir: str = "extractions"
) -> dict:
    """
    Parallel extraction with concurrency control using batching.
    Processes all URLs in batches to respect API limits.

    Args:
      urls: List of URLs to extract from (all processed in batches).
      original_user_query: Query string for metadata.
      url2tag: Dictionary mapping URLs to their bucket types.
      output_dir: Directory to save structured JSON.
    Returns:
      Categorized dict of extracted items.
    """
    logging.info(f"\nPhase 4: Extracting structured data from {len(urls)} URLs in batches...")
    os.makedirs(output_dir, exist_ok=True)
    
    # ————— fast concurrent extraction —————
    sem = asyncio.Semaphore(constants.MAX_GEMINI_PARALLEL)
    client = genai.Client(api_key=config.GEMINI_API_KEY)

    async def one(url):
        async with sem:
            # Run the synchronous extraction in a thread executor
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                None, 
                lambda: extract_data_from_single_url_sync(url, client)
            )

    t0 = time.perf_counter()
    out_lists = await asyncio.gather(*(one(u) for u in urls))
    elapsed = time.perf_counter() - t0
    logging.info(f"✓ Phase 4 – extracted {len(urls)} URLs in {elapsed:0.1f}s "
          f"({constants.MAX_GEMINI_PARALLEL} Gemini workers)")
    
    categorized = {k: [] for k in EXPECTED_CATEGORIES}
    total_items = 0
    
    for items in out_lists:
        for item in items:
            if not _is_recent(item.get("date")):
                # silently drop anything older than RECENT_YEARS
                continue
            
            # --- Add a parsed date for sorting ---
            # We'll parse the date string into a real date object.
            # We add this temporarily and will remove it before saving.
            try:
                # The 'fuzzy=True' helps parse incomplete dates like "2024-05"
                item['_parsed_date'] = dtparse.parse(item.get("date", ""), fuzzy=True)
            except (dtparse.ParserError, TypeError):
                # If date is invalid, default to a very old date so it goes to the bottom.
                item['_parsed_date'] = datetime.datetime(1970, 1, 1)
            
            # When normalising each item - use url2tag for type guessing
            item_type = (item.get("type") or url2tag.get(item.get("source_url"), "Other")) or "Other"
            item["type"] = item_type
            
            t = item.get("type", "Other")
            categorized.setdefault(t if t in EXPECTED_CATEGORIES else "Other", categorized["Other"]).append(item)
            total_items += 1

    # --- Sorting Logic ---
    # Now, iterate through each category and sort its list of items.
    print("    - Sorting extracted items by date...")
    for category in categorized:
        # Sort the list in-place.
        # `itemgetter` is slightly faster than a lambda function.
        # `reverse=True` puts the newest dates first.
        categorized[category].sort(key=itemgetter('_parsed_date'), reverse=True)
        
        # Clean up the temporary '_parsed_date' key from each item.
        for item in categorized[category]:
            del item['_parsed_date']

    # Prepare metadata and write output
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_title = re.sub(r'\W+', '_', (original_user_query or '')[:50]) or 'structured_extraction'
    filename = f"{timestamp}_{safe_title}.json"
    filepath = os.path.join(output_dir, filename)

    categorized = _pad_categories(categorized)

    urls_processed = len(urls)
    metadata = {
        "timestamp": timestamp,
        "original_query": original_user_query,
        "urls_processed": urls_processed,
        "total_items_extracted": total_items,
        "extraction_summary": {cat: len(lst) for cat, lst in categorized.items()}
    }
    output = {"metadata": metadata, "extracted_data": categorized, "processed_urls": urls}

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"✅ Structured extraction saved to {filepath}")
    print(f"📊 {total_items} items extracted:")
    for cat, lst in categorized.items():
        if lst:
            print(f"   - {cat}: {len(lst)}")

    return output

if __name__ == '__main__':
    sample_urls = [
        "https://www.coatingsworld.com/issues/2025-01-01/view_features/renovations-diy-drive-growth-in-architectural-coatings-market/",
        "https://www.paint.org/wp-content/uploads/dlm_uploads/2020/04/2020-ACA-Sustainability-Report-1.pdf"
    ]
    sample_url2tag = {url: "News" for url in sample_urls}  # Sample mapping
    extracted = asyncio.run(run_structured_extraction(sample_urls, 'Test Extraction', sample_url2tag))
    print("\nFinal extracted data:")
    print(json.dumps(extracted, indent=2)) 


================================================
File: src/phase5_final_synthesizer.py
================================================
# src/phase5_final_synthesizer.py

import os
import re
import datetime
import logging
from datetime import date
from google import genai
from google.genai import types
from src import config
from src import constants
import hashlib

def synthesize_final_report(
    original_user_query: str,
    intermediate_reports_text: list[str],
    all_original_urls: list[str],
    output_dir: str = "reports"
) -> str:
    """
    Consolidates intermediate sub-reports into a comprehensive Markdown report.
    """
    os.makedirs(output_dir, exist_ok=True)
    logging.info(f"\nPhase 5: Generating final report from {len(intermediate_reports_text)} intermediate documents...")

    if not intermediate_reports_text:
        return _create_fallback_report(original_user_query, output_dir, "No intermediate reports available")

    formatted_content = _format_intermediate_reports(intermediate_reports_text)
    if len(formatted_content) > 100_000:
        logging.warning("    - Warning: content truncated for context limit")
        formatted_content = formatted_content[:100_000] + "\n\n[Content truncated due to length...]"

    client = genai.Client(api_key=config.GEMINI_API_KEY)

    # Get current date for context
    current_date = date.today()
    current_year = current_date.year
    target_years = f"{current_year - constants.RECENT_YEARS + 1}-{current_year}"

    system_instruction = (
        f"You are the Chief Market Intelligence Officer for a global chemical company. Your audience is the CEO and the Board of Directors. Your task is to synthesize a collection of internal research briefings into a single, cohesive, and forward-looking executive report. Today is {current_date.strftime('%B %d, %Y')}.\n\n"
        
        "**MANDATE: Synthesize, Don't Summarize.**\n"
        "Do not simply concatenate the provided intermediate reports. Your value is in finding the 'golden threads' that connect them. Identify the overarching trends, reconcile conflicting data points, and expose the critical strategic narrative hidden within the details. For every fact, you must answer the business-critical question: **'So what?'**\n\n"
        
        "**STRATEGIC REPORT STRUCTURE:**\n"
        "Structure your brief using the following Markdown format. Each section must be analytical and focused on strategic implications.\n\n"
        
        "1.  **Executive Summary (The 3-Minute Briefing)**\n"
        "    - Start with the single most critical conclusion from this research.\n"
        "    - Present 2-3 key opportunities and threats that demand immediate leadership attention.\n"
        "    - Conclude with the bottom-line impact for our company.\n\n"
        
        "2.  **Market Trajectory & Headwinds**\n"
        "    - Synthesize market size and growth forecasts. Where are the pockets of growth, and where is the market stagnating?\n"
        "    - What are the primary economic or consumer behavior trends driving demand?\n\n"
        
        "3.  **Competitive Arena: Key Moves & Vulnerabilities**\n"
        "    - Analyze the strategic maneuvers of key competitors (e.g., PPG, AkzoNobel, BASF). Where are they investing? Where do they appear weak?\n"
        "    - Highlight any M&A activity or partnerships that could reshape the market.\n\n"
        
        "4.  **Technology & Innovation: The Next Frontier**\n"
        "    - What are the 1-2 breakthrough technologies that could disrupt the market (e.g., in sustainability, application, performance)?\n"
        "    - What does the patent landscape suggest about the future of coatings R&D?\n\n"
        
        "5.  **Regulatory & ESG: Risks and Opportunities**\n"
        "    - What new regulations or environmental standards pose the greatest risk to our current product portfolio?\n"
        "    - How can we leverage sustainability trends (e.g., circular economy, bio-based materials) into a competitive advantage?\n\n"
        
        "6.  **Actionable Strategic Recommendations**\n"
        "    - **This is the most important section.** Provide 3-5 specific, bold, and actionable recommendations.\n"
        "    - Frame them as clear directives. Example: 'Recommend allocating an additional $5M in R&D to develop a proprietary scuff-resistant resin to counter Competitor X's new product launch' instead of 'We should invest in R&D.'\n\n"
        
        "**EXECUTIVE COMMUNICATION PROTOCOL:**\n"
        "•   **Be Decisive & Direct:** Use strong, declarative sentences. Lead with the conclusion, then provide brief evidence. Avoid hedging language like 'it seems' or 'it could be'.\n"
        "•   **Prioritize Brevity:** Use bullet points for everything possible. Each section should be scannable in seconds. The entire report should be digestible in under 5 minutes.\n"
        "•   **Focus on Insight, Not Information:** Do not just list facts. State the insight derived from the fact. BAD: 'Company X launched a new product.' GOOD: 'Company X's new product launch directly threatens our market share in the premium segment, representing a potential 10% revenue risk.'\n"
        "•   **Quantify Everything:** Use all available figures, percentages, and timelines to support your analysis. Estimate when necessary, and state that it is an estimate.\n"
        "•   **Reference Integrity:** Do NOT include inline citations. A reference list of the source URLs will be appended automatically.\n\n"
        
        "**DELIVERABLE:** A polished, professional Markdown report that can be directly used for the company's strategic planning session."
    )

    # Combine system instruction with user instruction since Gemini only accepts "user" and "model" roles
    combined_instruction = (
        f"{system_instruction}\n\n"
        f"**RESEARCH OBJECTIVE:**\n{original_user_query}\n\n"
        f"**INTERMEDIATE REPORTS TO SYNTHESIZE ({len(intermediate_reports_text)} parts):**\n"
        f"{formatted_content}\n\n"
        "**TASK:** Create a comprehensive, executive-ready market intelligence report that "
        "addresses the research objective using the provided intermediate analysis."
    )

    contents = [
        types.Content(role="user", parts=[types.Part(text=combined_instruction)])
    ]

    config_obj = types.GenerateContentConfig(
        tools=[],
        safety_settings=[
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_MEDIUM_AND_ABOVE"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_MEDIUM_AND_ABOVE"),
        ],
        response_modalities=["TEXT"]
    )

    try:
        logging.info("    - Calling Gemini for final synthesis...")
        stream = client.models.generate_content_stream(
            model="gemini-2.5-flash-preview-05-20",
            contents=contents,
            config=config_obj,
        )
        final_text = "".join(chunk.text for chunk in stream).strip()

        removed_count = len(all_original_urls)
        if removed_count < 12:   # keeps report honest if data set is now small
            final_text = (
                f"> **Note ·**  After applying the {constants.RECENT_YEARS}-year "
                "freshness filter only "
                f"{removed_count} source URLs remained.\n\n"
            ) + final_text

        final_with_refs = _add_references_section(final_text, all_original_urls)
        filepath = _save_final_report(final_with_refs, original_user_query, output_dir)

        logging.info(f"✅ Final report saved to {filepath} ({len(final_with_refs):,} chars, {len(all_original_urls)} references)")
        return filepath

    except Exception as e:
        logging.error(f"❌ Gemini error: {e}")
        return _create_fallback_report(original_user_query, output_dir, str(e), intermediate_reports_text)

def _format_intermediate_reports(reports: list[str]) -> str:
    return "\n\n".join(
        f"\n\n{'='*60}\nINTERMEDIATE REPORT #{i+1}\n{'='*60}\n\n{r.strip()}"
        for i, r in enumerate(reports)
    )

def _add_references_section(report_md: str, urls: list[str]) -> str:
    if not urls:
        return report_md + "\n\n---\n\n## References\n\n_No URLs provided._\n"
    refs = "\n".join(f"{i+1}. {url}" for i, url in enumerate(urls))
    return report_md + f"\n\n---\n\n## References\n\n*Synthesized from {len(urls)} sources:*\n\n{refs}\n"

def _save_final_report(content: str, query: str, output_dir: str) -> str:
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    safe = hashlib.sha1(query.encode()).hexdigest()[:16]
    path = os.path.join(output_dir, f"{ts}_{safe}_FINAL_REPORT.md")
    with open(path, "w", encoding="utf‑8") as f:
        f.write(content)
    return path

def _create_fallback_report(query: str, output_dir: str, reason: str, reports: list[str] = None) -> str:
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    safe = hashlib.sha1(query.encode()).hexdigest()[:16]
    path = os.path.join(output_dir, f"{ts}_{safe}_FALLBACK_REPORT.md")
    content = f"# Report: {query}\n\n**Status:** Incomplete\n\n**Reason:** {reason}\n\n"
    if reports:
        content += "## Intermediate Reports\n\n" + "\n\n".join(f"### Report #{i+1}\n{r}" for i, r in enumerate(reports))
    with open(path, "w", encoding="utf‑8") as f:
        f.write(content)
    return path



================================================
File: src/rag_uploader.py
================================================
# src/rag_uploader.py
"""
RAG Uploader Module for Market Research Intelligence

This module handles the uploading of market research artifacts to a RAG (Retrieval-Augmented Generation) system.
It creates collections, converts content to PDF format, uploads documents, and manages chat contexts.

Key Features:
- Collection creation with custom system prompts
- PDF conversion for all document types
- Document upload with proper metadata
- Chat context management for queries
- Preprocessing instructions setup
- Parallel document uploads for improved performance
"""

import requests
import json
import time
import tempfile
import os
from io import BytesIO
from reportlab.lib.pagesizes import letter
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.enums import TA_LEFT
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from src import config

# =============================================================================
# GLOBAL VARIABLES
# =============================================================================

# NOTE: Chat context management has been moved to the database.
# The query_rag_collection function is now stateless.

# Thread lock for thread-safe operations
upload_lock = threading.Lock()

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def _convert_to_pdf(json_data: dict, document_name: str) -> str:
    """
    Convert JSON data to a formatted PDF file and return the file path.

    Args:
        json_data (dict): The data to convert to PDF
        document_name (str): Name for the document (used in title and filename)

    Returns:
        str: Path to the generated PDF file
    """
    # Create a temporary file
    temp_fd, temp_path = tempfile.mkstemp(suffix='.pdf', prefix=f"{document_name}_")
    os.close(temp_fd)  # Close the file descriptor

    # Create PDF document
    doc = SimpleDocTemplate(temp_path, pagesize=letter, topMargin=1*inch)
    styles = getSampleStyleSheet()

    # Custom styles
    title_style = ParagraphStyle(
        'CustomTitle',
        parent=styles['Heading1'],
        fontSize=16,
        spaceAfter=12,
        alignment=TA_LEFT
    )

    content_style = ParagraphStyle(
        'CustomContent',
        parent=styles['Normal'],
        fontSize=10,
        spaceAfter=6,
        alignment=TA_LEFT
    )

    # Build PDF content
    story = []

    # Add title
    story.append(Paragraph(f"Document: {document_name}", title_style))
    story.append(Spacer(1, 12))

    # Process the JSON data
    if 'content' in json_data:
        # For reports with content field
        content = json_data['content']
        # Split content into paragraphs for better formatting
        paragraphs = content.split('\n\n') if content else ['No content available']

        for para in paragraphs:
            if para.strip():
                # Escape HTML characters and handle markdown-like formatting
                clean_para = para.replace('&', '&').replace('<', '<').replace('>', '>')

                # Handle markdown bold formatting more carefully
                import re
                # Replace **text** with <b>text</b>
                clean_para = re.sub(r'\*\*(.*?)\*\*', r'<b>\1</b>', clean_para)
                # Replace single *text* with <i>text</i> (but not if it's part of **)
                clean_para = re.sub(r'(?<!\*)\*([^*]+?)\*(?!\*)', r'<i>\1</i>', clean_para)

                story.append(Paragraph(clean_para, content_style))
                story.append(Spacer(1, 6))
    elif 'items' in json_data:
        # For combined structured data with multiple items
        items = json_data['items']
        category = json_data.get('category', 'Items')

        story.append(Paragraph(f"Category: {category}", title_style))
        story.append(Spacer(1, 12))

        for i, item in enumerate(items, 1):
            story.append(Paragraph(f"<b>Item {i}:</b>", content_style))
            for key, value in item.items():
                if isinstance(value, str) and value.strip():
                    formatted_key = key.replace('_', ' ').title()
                    clean_value = value.replace('&', '&').replace('<', '<').replace('>', '>')
                    story.append(Paragraph(f"<b>{formatted_key}:</b> {clean_value}", content_style))
            story.append(Spacer(1, 8))
    else:
        # For structured data items
        for key, value in json_data.items():
            if key in ['title', 'type', 'summary', 'date', 'source_url']:
                formatted_key = key.replace('_', ' ').title()
                if isinstance(value, str):
                    clean_value = value.replace('&', '&').replace('<', '<').replace('>', '>')
                    story.append(Paragraph(f"<b>{formatted_key}:</b> {clean_value}", content_style))
                else:
                    story.append(Paragraph(f"<b>{formatted_key}:</b> {str(value)}", content_style))
                story.append(Spacer(1, 4))

    # Add metadata section
    story.append(Spacer(1, 12))
    story.append(Paragraph("Metadata:", title_style))

    # Add source type and other metadata
    for key, value in json_data.items():
        if key not in ['content', 'title', 'type', 'summary', 'date', 'source_url', 'items', 'category']:
            formatted_key = key.replace('_', ' ').title()
            clean_value = str(value).replace('&', '&').replace('<', '<').replace('>', '>')
            story.append(Paragraph(f"<b>{formatted_key}:</b> {clean_value}", content_style))

    # Build the PDF
    doc.build(story)

    return temp_path

def _combine_structured_data_by_category(extracted_data: dict) -> dict:
    """
    Combine structured data items by category into single documents.

    Args:
        extracted_data (dict): Dictionary with categories and their items

    Returns:
        dict: Combined documents by category
    """
    combined_docs = {}

    for category, items in extracted_data.items():
        if items:  # Only process categories that have items
            combined_docs[category.lower()] = {
                'category': category,
                'items': items,
                'source_type': f'combined_{category.lower()}',
                'item_count': len(items)
            }

    return combined_docs

# =============================================================================
# CORE API FUNCTIONS
# =============================================================================

def _create_rag_collection(collection_name: str, description: str):
    """
    Calls the /app/v2/CreateCollection endpoint using the actual API specification.

    Args:
        collection_name (str): Name for the new collection (without org prefix)
        description (str): Description of the collection

    Returns:
        dict: API response
    """
    headers = {
        "X-Api-Token": config.RAG_API_TOKEN,
        "x-orgId": config.RAG_API_ORG_ID,
        "X-Api-Org": config.RAG_API_ORG_ID
    }
    data = {
        "collectionName": collection_name,
        "collectionDescription": description,
        "jsonFields": "",  # Empty as shown in example
        "usertype": config.RAG_API_USER_TYPE,
        "base_language": "en",
        "source": "files",
        "model": "gpt-4o-mini",
        "response_language": "en"
    }
    url = f"{config.RAG_API_BASE_URL}/app/v2/CreateCollection"

    response = requests.post(url, headers=headers, data=data)
    if response.status_code != 200:
        print(f"❌ CreateCollection failed. Status: {response.status_code}")
        print(f"❌ Response: {response.text}")
    response.raise_for_status() # Raise an exception for HTTP errors
    return response.json()

def _update_system_prompt(collection_name: str):
    """
    Calls the /app/v2/UpdateSystemPrompt endpoint to set a custom system prompt for the collection.

    Args:
        collection_name (str): Name of the collection to update (with org prefix)

    Returns:
        dict: API response
    """
    headers = {
        "X-Api-Token": config.RAG_API_TOKEN,
        "x-orgId": config.RAG_API_ORG_ID,
        "X-Api-Org": config.RAG_API_ORG_ID
    }

    # Create a refined system prompt for market research intelligence
    system_prompt = """You are an expert Market Intelligence Analyst, a precision Q&A engine. Your knowledge is strictly and absolutely limited to the documents uploaded for this research job. Your purpose is to provide quick, reliable answers.

**Your Core Directives:**

1.  **Scope Limitation:** Answer ONLY from the provided documents. If the answer is not in the documents, you MUST state: "I cannot answer that question based on the provided documents." Do not use any external knowledge.

2.  **Answer First, Then Cite:** Respond directly and concisely to the user's question first. Then, provide precise citations in the format `[document_name]` that support your answer.

3.  **Synthesize, Don't Recite:** Do not just quote passages. Synthesize information from multiple sources to form a complete, clear answer. Use bullet points for clarity.

4.  **No Small Talk:** Do not greet the user or use conversational filler. But if a the user says hello or who are you, then you can say hello or I am a market intelligence analyst. Do not be rude or sarcastic. Be professional and friendly.
"""

    data = {
        "new_prompt": system_prompt,
        "partition_name": collection_name,
        "username": "mbhimrajka@supervity.ai"
    }

    url = f"{config.RAG_API_BASE_URL}/app/v2/UpdateSystemPrompt"

    response = requests.post(url, headers=headers, data=data)
    if response.status_code != 200:
        print(f"❌ UpdateSystemPrompt failed. Status: {response.status_code}")
        print(f"❌ Response: {response.text}")
    response.raise_for_status()
    return response.json()

def _set_preprocess_instructions(collection_name: str):
    """
    Calls the /app/v2/PreprocessInstruct endpoint to set preprocessing instructions for the collection.

    Args:
        collection_name (str): Name of the collection to configure (with org prefix)

    Returns:
        dict: API response
    """
    headers = {
        "X-Api-Token": config.RAG_API_TOKEN,
        "x-orgId": config.RAG_API_ORG_ID,
        "X-Api-Org": config.RAG_API_ORG_ID
    }

    # Create preprocessing instructions for market research data
    preprocess_instructions = """Please find below the key preprocessing guidelines for market research intelligence analysis:

1. **Document Structure**: Identify document types (final reports, intermediate reports, news articles, patents, conference papers) and structure the analysis accordingly.

2. **Data Categorization**: Organize information into relevant categories:
   - Market trends and forecasts
   - Competitive intelligence
   - Technological innovations
   - Regulatory changes
   - Industry dynamics

3. **Key Information Extraction**: Focus on extracting:
   - Market size and growth projections
   - Key players and market share data
   - Emerging technologies and innovations
   - Geographic market insights
   - Regulatory and compliance factors
   - Investment and funding activities

4. **Source Credibility**: Prioritize information based on source reliability and recency of data.

5. **Cross-Reference Analysis**: Identify patterns and correlations across multiple sources and document types.

6. **Date Sensitivity**: Pay attention to publication dates and ensure temporal relevance of insights.

7. **Quantitative Data**: Extract and highlight numerical data, percentages, market valuations, and statistical information.

8. **Actionable Intelligence**: Focus on information that provides strategic value and business decision support.

9. **Citation Preparation**: Maintain document source information for proper attribution in responses.

10. **Context Preservation**: Maintain the relationship between the original research query and the extracted information."""

    data = {
        "partition_name": collection_name,
        "username": "mbhimrajka@supervity.ai",
        "preprocess_instruct": preprocess_instructions
    }

    url = f"{config.RAG_API_BASE_URL}/app/v2/PreprocessInstruct"

    response = requests.post(url, headers=headers, data=data)
    if response.status_code != 200:
        print(f"❌ PreprocessInstruct failed. Status: {response.status_code}")
        print(f"❌ Response: {response.text}")
    response.raise_for_status()
    return response.json()

def _upload_document(collection_name: str, document_name: str, json_data: dict):
    """
    Calls the /app/v2/UploadDocument endpoint using the actual API specification.
    Converts content to PDF before uploading.

    Args:
        collection_name (str): Name of the collection to upload to (with org prefix)
        document_name (str): Name for the document
        json_data (dict): Data to convert and upload

    Returns:
        dict: API response with success/failure info
    """
    headers = {
        "X-Api-Token": config.RAG_API_TOKEN,
        "x-orgId": config.RAG_API_ORG_ID,
        "X-Api-Org": config.RAG_API_ORG_ID
    }

    # Convert content to PDF
    pdf_file_path = _convert_to_pdf(json_data, document_name)

    try:
        # Upload the PDF file
        with open(pdf_file_path, 'rb') as pdf_file:
            files = {
                'document': (f"{document_name}.pdf", pdf_file, 'application/pdf')
            }
            data = {
                "collectionName": collection_name,
                "jsonData": "",  # Empty as shown in example
                "documentName": document_name,
                "usertype": config.RAG_API_USER_TYPE,
                "useOCR": "false"
            }

            url = f"{config.RAG_API_BASE_URL}/app/v2/UploadDocument"

            # Debug: Show file size
            pdf_file.seek(0, 2)  # Go to end
            file_size = pdf_file.tell()
            pdf_file.seek(0)  # Go back to start

            with upload_lock:
                print(f"   → Uploading PDF: {document_name}.pdf ({file_size:,} bytes)")

            response = requests.post(url, headers=headers, data=data, files=files)

            if response.status_code != 200:
                with upload_lock:
                    print(f"   ❌ Upload failed for {document_name}. Status: {response.status_code}")
                    print(f"   ❌ Response: {response.text}")
                return {"success": False, "document_name": document_name, "error": response.text}

            with upload_lock:
                print(f"   ✅ Upload successful: {document_name}")
            return {"success": True, "document_name": document_name, "response": response.json()}

    except Exception as e:
        with upload_lock:
            print(f"   ❌ Upload failed for {document_name}. Error: {e}")
        return {"success": False, "document_name": document_name, "error": str(e)}
    finally:
        # Clean up the temporary PDF file
        if os.path.exists(pdf_file_path):
            os.remove(pdf_file_path)

def query_rag_collection(collection_name: str, question: str, current_chat_context: str = "") -> dict:
    """
    Calls the /app/v2/QueryDocument endpoint.
    Manages chat context via passed-in arguments, it is STATELESS.

    Args:
        collection_name (str): Name of the collection to query
        question (str): Question to ask the RAG system
        current_chat_context (str): The conversation history from the database.

    Returns:
        dict: The full RAG response payload from the API.
    """
    config.assert_rag_env()
    print(f"Querying RAG collection '{collection_name}' with question: '{question}'")
    print(f"   -> Sending chat context (length: {len(current_chat_context)} chars)")

    headers = {
        "X-Api-Token": config.RAG_API_TOKEN,
        "x-orgId": config.RAG_API_ORG_ID,
        "X-Api-Org": config.RAG_API_ORG_ID
    }
    
    data = {
        "question": question,
        "collectionName": collection_name,
        "jsonData": "",
        "documentName": "",
        "usertype": config.RAG_API_USER_TYPE,
        "chat_context": current_chat_context
    }

    url = f"{config.RAG_API_BASE_URL}/app/v2/QueryDocument"

    try:
        response = requests.post(url, headers=headers, data=data)
        response.raise_for_status()
        result = response.json()
        
        # The function is now stateless. It just returns the result.
        # The caller is responsible for updating the context.
        print(f"   -> Received response from RAG API.")
        return result
        
    except requests.exceptions.HTTPError as e:
        print(f"Error querying RAG. Status: {e.response.status_code}, Body: {e.response.text}")
        raise e


# =============================================================================
# MAIN ORCHESTRATION FUNCTION
# =============================================================================

def upload_artifacts_to_rag(job_id: str, artifacts: dict):
    """
    Main function to upload research artifacts to the RAG system with parallel uploads.

    This function orchestrates the complete upload process:
    1. Creates a new collection
    2. Sets up system prompt and preprocessing instructions
    3. Uploads final report, intermediate reports, and structured data in parallel
    4. Initializes chat context for future queries

    Args:
        job_id (str): Unique identifier for the research job
        artifacts (dict): Dictionary containing all research artifacts

    Returns:
        str or None: Collection name if successful, None if failed
    """
    try:
        config.assert_rag_env()
        print(f"--- Starting RAG Upload for Job ID: {job_id} ---")

        # 1. Create a new collection for this job
        base_collection_name = f"research_job_{job_id.replace('-', '_')}"
        collection_description = f"Artifacts for research query: {artifacts['original_query'][:100]}"

        _create_rag_collection(base_collection_name, collection_description)
        print(f"-> RAG: Created collection '{base_collection_name}'.")

        # 2. For subsequent operations, use the full collection name with org prefix
        full_collection_name = f"{config.RAG_API_ORG_ID}_{base_collection_name}"

        # 3. Configure collection settings
        _update_system_prompt(full_collection_name)
        print(f"-> RAG: Updated system prompt for '{full_collection_name}'.")

        _set_preprocess_instructions(full_collection_name)
        print(f"-> RAG: Set preprocessing instructions for '{full_collection_name}'.")

        # 4. Prepare all upload tasks
        upload_tasks = []

        # Add final report to upload tasks
        upload_tasks.append(("final_report", {
            "content": artifacts['final_report_markdown'],
            "source_type": "final_report"
        }))

        # Add job-specific intermediate reports to upload tasks
        if 'intermediate_reports' in artifacts and artifacts['intermediate_reports']:
            for i, intermediate_report in enumerate(artifacts['intermediate_reports']):
                upload_tasks.append((f"intermediate_report_{i}", {
                    "content": intermediate_report,
                    "source_type": "intermediate_report",
                    "report_index": i
                }))

        # Combine structured data by category and add to upload tasks
        extracted_data = artifacts['extracted_data']
        combined_docs = _combine_structured_data_by_category(extracted_data)

        for category, combined_data in combined_docs.items():
            upload_tasks.append((f"combined_{category}", combined_data))

        # 5. Execute all uploads in parallel
        print(f"-> RAG: Starting parallel upload of {len(upload_tasks)} documents...")

        successful_uploads = 0
        failed_uploads = 0

        with ThreadPoolExecutor(max_workers=5) as executor:  # Limit concurrent uploads
            # Submit all upload tasks
            future_to_task = {
                executor.submit(_upload_document, full_collection_name, task_name, task_data): (task_name, task_data)
                for task_name, task_data in upload_tasks
            }

            # Process completed uploads
            for future in as_completed(future_to_task):
                task_name, task_data = future_to_task[future]
                try:
                    result = future.result()
                    if result.get("success"):
                        successful_uploads += 1
                    else:
                        failed_uploads += 1
                        print(f"   ❌ Failed to upload {task_name}: {result.get('error', 'Unknown error')}")
                except Exception as e:
                    failed_uploads += 1
                    print(f"   ❌ Exception during upload of {task_name}: {e}")

        # 6. Chat context is now managed by the database, no initialization needed

        # 7. Summary
        total_items = len(combined_docs)
        intermediate_count = len(artifacts.get('intermediate_reports', []))

        print(f"--- RAG Upload Complete ---")
        print(f"   Collection: '{full_collection_name}'")
        print(f"   Successful uploads: {successful_uploads}")
        print(f"   Failed uploads: {failed_uploads}")
        print(f"   Final report: 1, Intermediate reports: {intermediate_count}, Combined categories: {total_items}")
        print(f"--- Total documents uploaded: {successful_uploads} ---")

        if failed_uploads > 0:
            print(f"⚠️  Warning: {failed_uploads} uploads failed")

        return full_collection_name if successful_uploads > 0 else None

    except Exception as e:
        print(f"❌ RAG Upload Failed for Job ID: {job_id}. Error: {e}")
        return None

# =============================================================================
#   🧪 REALISTIC TESTING WITH ACTUAL PIPELINE FILES
# =============================================================================

def main():
    """
    Test the entire RAG pipeline using actual files from the market research pipeline.
    
    This function:
    1. Reads a real final report from the reports/ directory
    2. Loads job-specific intermediate reports from reports/intermediate_reports/
    3. Loads real extracted data from extractions/ directory 
    4. Tests the complete upload and query workflow with realistic data
    """
    import json
    import glob
    import os
    from pathlib import Path
    import re
    
    print("🧪 Testing RAG Pipeline with Actual Market Research Files")
    print("=" * 70)
    
    # ─────────────────────────────────────────────────────────────────────────────────────────
    # 1. Load actual final report from reports directory
    # ─────────────────────────────────────────────────────────────────────────────────────────
    
    reports_dir = Path("reports")
    final_reports = list(reports_dir.glob("*FINAL_REPORT.md"))
    
    if not final_reports:
        print("❌ No final reports found in reports/ directory")
        return
    
    # Use the most recent final report
    latest_report = max(final_reports, key=lambda x: x.stat().st_mtime)
    print(f"📄 Using final report: {latest_report.name}")
    
    try:
        with open(latest_report, 'r', encoding='utf-8') as f:
            final_report_content = f.read()
        print(f"   ✓ Loaded final report ({len(final_report_content):,} characters)")
    except Exception as e:
        print(f"❌ Error reading final report: {e}")
        return
    
    # ─────────────────────────────────────────────────────────────────────────────────────────
    # 2. Extract job ID from filename and load job-specific intermediate reports
    # ─────────────────────────────────────────────────────────────────────────────────────────
    
    # Extract UUID from filename (e.g., "20250616_114604_e8ba587e71d8fe26_FINAL_REPORT.md")
    uuid_match = re.search(r'([a-f0-9]{16})', latest_report.name)
    if uuid_match:
        job_uuid = uuid_match.group(1)
        job_id = f"{job_uuid}-71d8-fe26-1234-123456789abc"  # Extend to full UUID format
    else:
        job_id = "test-pipeline-12345678-1234-5678-9abc-123456789def"
    
    print(f"🔑 Using Job ID: {job_id}")
    print(f"🔍 Looking for intermediate reports with UUID: {job_uuid}")
    
    # Load only job-specific intermediate reports
    intermediate_dir = reports_dir / "intermediate_reports"
    intermediate_files = list(intermediate_dir.glob(f"*{job_uuid}*.md"))  # Filter by job UUID
    
    intermediate_reports = []
    for file_path in intermediate_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                intermediate_reports.append(content)
            print(f"   ✓ Loaded job-specific intermediate report: {file_path.name}")
        except Exception as e:
            print(f"   ⚠️  Failed to load {file_path.name}: {e}")
    
    print(f"📊 Loaded {len(intermediate_reports)} job-specific intermediate reports")
    
    # ─────────────────────────────────────────────────────────────────────────────────────────
    # 3. Load actual extracted data from JSON files
    # ─────────────────────────────────────────────────────────────────────────────────────────
    
    extractions_dir = Path("extractions")
    extraction_files = list(extractions_dir.glob("*.json"))
    
    if not extraction_files:
        print("❌ No extraction files found in extractions/ directory")
        return
    
    # Use the most recent extraction file
    latest_extraction = max(extraction_files, key=lambda x: x.stat().st_mtime)
    print(f"📊 Using extraction data: {latest_extraction.name}")
    
    try:
        with open(latest_extraction, 'r', encoding='utf-8') as f:
            extraction_data = json.load(f)
        
        # Validate the structure
        if 'extracted_data' not in extraction_data:
            print("❌ Invalid extraction file structure - missing 'extracted_data'")
            return
            
        extracted_items = extraction_data['extracted_data']
        metadata = extraction_data.get('metadata', {})
        
        print(f"   ✓ Loaded extraction data with categories: {list(extracted_items.keys())}")
        print(f"   ✓ Metadata: {metadata.get('total_items_extracted', 'unknown')} total items")
        
        # Show category breakdown
        for category, items in extracted_items.items():
            print(f"     - {category}: {len(items)} items")
        
    except Exception as e:
        print(f"❌ Error loading extraction data: {e}")
        return
    
    # ─────────────────────────────────────────────────────────────────────────────────────────
    # 4. Create realistic artifacts structure
    # ─────────────────────────────────────────────────────────────────────────────────────────
    
    # Get original query from metadata or use a default
    original_query = metadata.get('original_query', 
        """Show me the latest innovations in Weatherability of Decorative Coatings.
What trends are emerging in the Sustainability of industrial coatings in 2025?
Find recent conferences or Patents discussing Scuff-Resistance in coatings.

Search tags/topics - Product, coating, architectural or similar.""")
    
    artifacts = {
        'original_query': original_query,
        'final_report_markdown': final_report_content,
        'intermediate_reports': intermediate_reports,
        'extracted_data': extracted_items,
        'metadata': metadata
    }
    
    print(f"\n📦 Artifacts Summary:")
    print(f"   • Final Report: {len(final_report_content):,} characters")
    print(f"   • Job-Specific Intermediate Reports: {len(intermediate_reports)} files")
    print(f"   • Extracted Data Categories: {len(extracted_items)}")
    for category, items in extracted_items.items():
        print(f"     - {category}: {len(items)} items → will be combined into 1 document")
    
    # ─────────────────────────────────────────────────────────────────────────────────────────
    # 5. Test the complete upload pipeline
    # ─────────────────────────────────────────────────────────────────────────────────────────
    
    print(f"\n🚀 Starting RAG Upload Pipeline...")
    collection_name = upload_artifacts_to_rag(job_id, artifacts)
    
    if not collection_name:
        print("❌ RAG upload failed!")
        return
    
    # ─────────────────────────────────────────────────────────────────────────────────────────
    # 6. Test query functionality with business-relevant questions
    # ─────────────────────────────────────────────────────────────────────────────────────────
    
    test_queries = [
        "What are the latest innovations in weatherability for decorative coatings?",
        "What sustainability trends are emerging in industrial coatings for 2025?",
        "Which companies are leading in scuff-resistant coating technologies?",
        "What are the key patents and conferences related to coating durability?",
        "How is the global coatings market projected to grow through 2032?"
    ]
    
    print(f"\n🔍 Testing Query Functionality...")
    print("-" * 50)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{i}. Query: {query}")
        response = query_rag_collection(collection_name, query)
        
        if response:
            # Truncate long responses for readability
            response_text = response.get('response', str(response))
            display_response = response_text[:300] + "..." if len(response_text) > 300 else response_text
            print(f"   ✓ Response: {display_response}")
        else:
            print(f"   ❌ Query failed")
        
        # Small delay between queries
        time.sleep(1)
    
    print(f"\n✅ RAG Pipeline Test Complete!")
    print(f"   Collection: {collection_name}")
    print(f"   Total Queries: {len(test_queries)}")
    print("=" * 70)


if __name__ == "__main__":
    main() 


================================================
File: src/tasks.py
================================================
# File: src/tasks.py (NEW FILE)
import asyncio
import logging
from celery_worker import celery_app # Import our Celery app instance

# --- All the imports from the original run_and_store_results function ---
from database.session import SessionLocal
from database.models import Job as DBJob
from src.main import execute_research_pipeline
from src.rag_uploader import upload_artifacts_to_rag
import threading

# Define the task using the @celery_app.task decorator
@celery_app.task(name="run_research_pipeline_task")
def run_research_pipeline_task(job_id: str, query: str, should_upload_to_rag: bool):
    """
    This is the Celery task that executes the full research pipeline.
    It's the same logic as the old background task, but now it runs in a Celery worker.
    """
    logging.info(f"Celery task started for job_id: {job_id}")
    db = SessionLocal()
    try:
        job = db.query(DBJob).filter(DBJob.id == job_id).first()
        if not job:
            logging.error(f"Job {job_id} not found in DB for Celery task. Aborting.")
            return

        logging.info(f"Job {job_id}: Starting research pipeline...")
        job.status = 'running'
        job.job_stage = 'initializing'
        job.job_progress = 5
        db.commit()

        # --- This status callback is the same as before ---
        async def update_status_in_db(stage: str = None, progress: int = None, message: str = None):
            # This needs its own DB session because it's in an async context
            db_session = SessionLocal()
            try:
                job_to_update = db_session.query(DBJob).filter(DBJob.id == job_id).first()
                if job_to_update:
                    if stage: job_to_update.job_stage = stage
                    if progress: job_to_update.job_progress = progress
                    if message:
                        if job_to_update.logs is None: job_to_update.logs = []
                        job_to_update.logs = job_to_update.logs + [message]
                    db_session.commit()
            finally:
                db_session.close()
            await asyncio.sleep(0)

        # Run the main research pipeline
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            result_data = loop.run_until_complete(
                execute_research_pipeline(query, update_status_in_db)
            )
        finally:
            loop.close()
        
        logging.info(f"Job {job_id}: Pipeline completed. Marking as COMPLETED immediately.")
        job.result = result_data
        job.status = 'completed'
        job.job_stage = 'finished'
        job.job_progress = 100
        
        if should_upload_to_rag:
            job.upload_to_rag = True
            job.rag_status = 'pending_upload'
        else:
            job.upload_to_rag = False
            job.rag_status = 'not_requested'
        
        db.commit()
        logging.info(f"Job {job_id}: Status set to COMPLETED in DB.")
        
        # Start RAG upload in a background thread (same logic)
        if should_upload_to_rag:
            logging.info(f"Job {job_id}: Starting RAG upload in background thread...")
            # This part remains identical, using a thread within the Celery worker
            def run_rag_upload():
                try:
                    collection_name = upload_artifacts_to_rag(job_id, result_data)
                    with SessionLocal() as rag_db:
                        rag_job = rag_db.query(DBJob).filter(DBJob.id == job_id).first()
                        if rag_job:
                            if collection_name:
                                rag_job.rag_collection_name = collection_name
                                rag_job.rag_status = 'uploaded'
                            else:
                                rag_job.rag_status = 'failed'
                                rag_job.rag_error = 'RAG upload returned no collection name'
                            rag_db.commit()
                except Exception as rag_error:
                    logging.error(f"Job {job_id}: RAG upload failed.", exc_info=True)
                    with SessionLocal() as rag_db:
                        rag_job = rag_db.query(DBJob).filter(DBJob.id == job_id).first()
                        if rag_job:
                            rag_job.rag_status = 'failed'
                            rag_job.rag_error = str(rag_error)
                            rag_db.commit()
            
            rag_thread = threading.Thread(target=run_rag_upload, daemon=True)
            rag_thread.start()
    
    except Exception as e:
        logging.error(f"Job {job_id}: Celery task failed with an unhandled exception.", exc_info=True)
        job.status = 'failed'
        job.job_stage = 'error'
        job.job_progress = 0
        job.result = {"error": str(e)}
        db.commit()
    finally:
        db.close() 


